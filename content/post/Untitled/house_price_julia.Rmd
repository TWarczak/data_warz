---
title: "Predict housing prices in Austin TX with tidymodels and xgboost"
author: Julia Silge
date: '2021-08-15'
slug: austin-housing
categories:
  - rstats
  - tidymodels
tags:
  - rstats
  - tidymodels
subtitle: ''
summary: "More xgboost with tidymodels! Learn about feature engineering to incorporate text information as indicator variables for boosted trees."
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: true
projects: []
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE, 
                      echo = TRUE, dpi = 300, cache.lazy = FALSE,
                      tidy = "styler", fig.width = 8, fig.height = 5)
library(scales)
library(tidyverse)
library(finetune)

#update_geom_defaults("rect", list(fill = "midnightblue", alpha = 0.8))
```

This is the latest in my series of
[screencasts](https://juliasilge.com/category/tidymodels/) demonstrating how to use
the [tidymodels](https://www.tidymodels.org/) packages, from just getting started to
tuning more complex models. My screencasts lately have focused on xgboost as I have
participated in
[SLICED](https://www.notion.so/SLICED-Show-c7bd26356e3a42279e2dfbafb0480073), a
competitive data science streaming show. This past week were the semifinals, where we
competed to predict prices of homes in Austin, TX. `r emo::ji("house")` One of the
more interesting available variables for this dataset was the text description of the
real estate listings, so let's walk through one way to incorporate text information
with boosted tree modeling.

```{r, echo=FALSE}
#blogdown::shortcode("youtube", "1LEW8APSOJo")
```

</br>

Here is the code I used in the video, for those who prefer reading instead of or in
addition to video.

## Explore data

Our modeling goal is to predict [the price (binned) for homes in Austin,
TX](https://www.kaggle.com/c/sliced-s01e11-semifinals/) given features about the real
estate listing. This is a multiclass classification challenge, where we needed to
submit a probability for each home being in each `priceRange` bin. The main data set
provided is in a CSV file called `training.csv`.

```{r}
train_raw <- read_csv("~/Documents/R/data_science_projects/SLICED/s01e11_house_price/train.csv")

holdout <- read_csv("~/Documents/R/data_science_projects/SLICED/s01e11_house_price/test.csv")

table(train_raw$priceRange)
```

You can watch [this week's full episode of
SLICED](https://www.twitch.tv/videos/1114553508) to see lots of exploratory data
analysis and visualization of this dataset, but let's just make a few data
visualization for context in this blog post.

How is price distributed across Austin?

```{r, fig.width=6.5}
library(ggmap)
```

```{r fig.height=4, fig.width=4, warning=FALSE, message=FALSE}

qmplot(longitude, latitude, data = train_raw, 
       color = priceRange, darken = .4, alpha = I(.6))


```

```{r fig.height=8, fig.width=8}
q <- train_raw %>%
     mutate(priceRange_bin = parse_number(priceRange))
#table(q$priceRange)
#scico::scico_palette_show(palettes = scico_palette_names())

pal <- colorNumeric(palette = scico(6, palette = 'buda'),
                    domain = q$priceRange_bin)

m <- leaflet(q) %>% 
     addTiles() %>% 
     addCircles(lng = ~longitude, lat = ~latitude, 
                color = ~pal(priceRange_bin), opacity = 0.8) %>% 
     setView(lng = -97.8, lat = 30.3, zoom = 9 ) %>% 
     addProviderTiles("CartoDB.DarkMatterNoLabels")%>%
     addProviderTiles("Stamen.TonerLines") %>%
     addProviderTiles("Stamen.TonerLabels") %>% 
     addLegend("bottomright", 
               pal       = pal, 
               values    = ~priceRange_bin,
               title     = "Price", 
               labFormat = labelFormat(prefix = "$"),
               opacity   = 1)
m
```

Let's look at this distribution and compare it to some other variables available in
the dataset. We can create a little plotting function [using
`{{}}`](https://dplyr.tidyverse.org/articles/programming.html#indirection) to quickly
iterate through, and put them together with
[patchwork](https://patchwork.data-imaginist.com/).

```{r, fig.width=10, fig.height=14}
plot_hex <- function(var, title) {
  q %>%
    ggplot(aes(longitude, latitude, z = {{var}})) +
    stat_summary_hex(bins = 40) +
    scico::scale_fill_scico(palette = 'imola') +
    labs(fill = "Avg", title = title) +
    ggdark::dark_theme_minimal()  
}

# names(q)
library(patchwork)
(plot_hex(priceRange_bin,"Price") + plot_hex(avgSchoolRating, "School rating")) /
(plot_hex(yearBuilt,"Year built") + plot_hex(log(lotSizeSqFt),"Lot size (log)"))/
(plot_hex(garageSpaces,"Garages") + plot_hex(MedianStudentsPerTeacher, "Median Student/Teacher")) / 
(plot_hex(numOfBathrooms, "Bathrooms") + plot_hex(numOfBedrooms,"Bedrooms")) 

```

```{r}
plot_hex(priceRange_bin,"Price") +
  facet_wrap(~city)

plot_hex(priceRange_bin,"Price") +
  facet_wrap(~hasSpa)

plot_hex(numOfPatioAndPorchFeatures, "# Patio Features")
```

Notice the east/west gradients as well as the radial changes. I went to grad school
in Austin and this all look very familiar to me!

```{r fig.width=10, fig.height=10}
dense <- q %>%
         select(-c(uid, city, description, homeType, 
                   hasSpa, priceRange_bin, yearBuilt)) %>%
         pivot_longer(cols = c(garageSpaces, avgSchoolRating, MedianStudentsPerTeacher,
                               numOfBathrooms, numOfBedrooms, longitude, latitude), 
                      names_to = "feature", 
                      values_to = "value")

ggplot(dense, aes(value, fill = priceRange)) +
  geom_density(alpha = .5) +
  facet_wrap(~ feature, scales = "free") +
  labs(title = "Numeric features impacting priceRange") +
  ggdark::dark_theme_minimal() +
  theme(legend.position = c(0.45, 0.2),
        legend.background = element_rect(fill = "black", color = "white"))
```

## Finding words related to price

The `description` variable contains text from each real estate listing. We could try
to use the text features directly in modeling, [as described in our
book](https://smltar.com/), but I've found that often isn't great for boosted tree
models (which tend to be what works best overall in an environment like SLICED).
Let's walk through another option which may work better in some situations, which is
to use some separate analysis to identify important words and then create dummy
variables indicating whether any given listing has those words.

Let's start by tidying the `description` text.

```{r}
library(tidytext)

austin_tidy <- 
  train_raw %>%
  mutate(priceRange = parse_number(priceRange) + 100000) %>%
  unnest_tokens(word, description) %>%
  anti_join(get_stopwords()) # remove stop words

austin_tidy %>%
  count(word, sort = TRUE)
```

Next, let's compute word frequencies per price range for the top 100 words.

```{r}
#dim(q)
top_words <- 
  austin_tidy %>%
  count(word, sort = TRUE) %>%
  filter(!word %in% as.character(1:5)) %>% # removing numbers
  slice_max(n, n = 100) %>% 
  pull(word)

word_freqs <- 
  austin_tidy %>%
  count(word, priceRange) %>%
  complete(word, priceRange, fill = list(n = 0)) %>%
  group_by(priceRange) %>%
  mutate(price_total = sum(n),
         proportion = n / price_total) %>%
  ungroup() %>%
  filter(word %in% top_words)

word_freqs
```

Now let's use modeling to find the words that are **increasing** with price and those
that are **decreasing** with price.

```{r}
word_mods <-
  word_freqs %>%
  nest(data = c(priceRange, n, price_total, proportion)) %>%
  mutate(model = map(data, ~ glm(cbind(n, price_total) ~ priceRange, ., 
                                 family = "binomial")),
         model = map(model, tidy)) %>%
  unnest(model) %>%
  filter(term == "priceRange") %>%
  mutate(p.value = p.adjust(p.value)) %>%
  arrange(-estimate)

word_mods
```

These are the words that we'd like to try to detect and use in feature engineering
for our xgboost model, rather than using all the text tokens as features
individually.

```{r}
higher_words <-
  word_mods %>%
  filter(p.value < 0.05) %>%
  slice_max(estimate, n = 12) %>%
  pull(word)

lower_words <- 
  word_mods %>%
  filter(p.value < 0.05) %>%
  slice_max(-estimate, n = 12) %>%
  pull(word)
```

We can look at these changes with price directly. For example, these are the words
most associated with price decrease.

```{r, fig.width=12, fig.height=14}
high <- word_freqs %>% 
  filter(word %in% higher_words) %>%
  ggplot(aes(priceRange, proportion, color = word)) +
  geom_line(size = 2.5, alpha = 0.7, show.legend = FALSE) +
  facet_wrap(vars(word), scales = "free") +
  scale_x_continuous(labels = scales::dollar) +
  scale_y_continuous(labels = scales::percent, limits = c(0, NA)) +
  labs(x = NULL, y = "% of word in home description at price",
       title = "12 Expensive Words") +
  ggdark::dark_theme_minimal() 

low <- word_freqs %>% 
  filter(word %in% lower_words) %>%
  ggplot(aes(priceRange, proportion, color = word)) +
  geom_line(size = 2.5, alpha = 0.7, show.legend = FALSE) +
  facet_wrap(vars(word), scales = "free") +
  scale_x_continuous(labels = scales::dollar) +
  scale_y_continuous(labels = scales::percent, limits = c(0, NA)) +
  labs(x = NULL, y = "% of word in home description at price",
       title = "12 Cheap Words") +
  ggdark::dark_theme_minimal() 

high/low
```

Cheaper houses are "great" but not expensive houses, and apparently you don't need to
mention the location ("close", "minutes", "location") of more expensive houses.

## Build a model

Let's start our modeling by setting up our "data budget", as well as the metrics
(this challenge was evaluate on multiclass log loss).

```{r}
# Make 75/12.5/12.5 Train/Validate/Test split
set.seed(333)
splits  <- initial_split(train_raw, prop = 0.75, strata = priceRange)

train <- training(splits)
other  <- testing(splits)

splits2 <- initial_split(other, prop = 0.50, strata = priceRange)
validation <- training(splits2)
test <- testing(splits2)

print(splits)
print(splits2)
```

For feature engineering, let's use basically everything in the dataset (aside from
`city`, which was not a very useful variable) and [create dummy or indicator
variables using
`step_regex()`](https://recipes.tidymodels.org/reference/step_regex.html). The idea
here is that we will detect whether these words associated with low/high price are
there and create a yes/no variable indicating their presence or absence.

```{r}
higher_words_25 <- word_mods %>%
                   filter(p.value < 0.05) %>%
                   slice_max(estimate, n = 25) %>%
                   pull(word)

lower_words_25 <- word_mods %>%
                  filter(p.value < 0.05) %>%
                  slice_max(-estimate, n = 25) %>%
                  pull(word)

higher_pat <- glue::glue_collapse(higher_words_25, sep = "|")
lower_pat <- glue::glue_collapse(lower_words_25, sep = "|")

rec_austin <-
  recipe(priceRange ~ ., data = train) %>%
  step_regex(description, pattern = higher_pat, result = "high_price_words") %>%
  step_regex(description, pattern = lower_pat, result = "low_price_words") %>%
  step_rm(c(uid, city, description, hasSpa, numOfPatioAndPorchFeatures)) %>%
  step_novel(homeType) %>% 
  # assign novel factor level if new data has homeType category unseen in train data
  step_unknown(homeType) %>% 
  # assign missing value to level "unknown"
  step_other(homeType, threshold = 0.02) %>%
  # low occuring values get pooled into "other" categoory
  step_dummy(all_nominal_predictors()) %>%
  step_nzv(all_predictors()) %>% 
  # removes highly sparse and unbalanced predictors
  prep()

```

```{r}
autin_training <- rec_austin %>% 
                  juice() %>% 
                  select(priceRange, everything()) # put priceRange as 1st column

autin_validation <- bake(rec_austin, new_data = validation) %>% 
                    select(priceRange, everything()) # put priceRange as 1st column

autin_test <- bake(rec_austin, new_data = test) %>% 
              select(priceRange, everything()) # put priceRange as 1st column

id_holdout <- holdout %>% select(uid) 
churn_holdout <- bake(priceRange, new_data = holdout)
```

Now let's create a tunable xgboost model specification, tuning a lot of the important
model hyperparameters, and combine it with our feature engineering recipe in a
`workflow()`. We can also create a custom `xgb_grid` to specify what parameters I
want to try out, like not-too-small learning rate, avoiding tree stubs, etc. I chose
this parameter grid to get reasonable performance in a reasonable amount of tuning
time.

```{r}
  dir.create("../s01e11_house_price/data") # local

write_csv(churn_training, 
          "../s01e11_house_price/data/churn_training.csv", 
          col_names = FALSE)
write_csv(churn_validation, 
          "../s01e11_house_price/data/churn_validation.csv", 
          col_names = FALSE)
write_csv(churn_test %>% select(-priceRange), 
          "../s01e11_house_price/data/churn_test.csv", 
          col_names = FALSE)
write_csv(churn_test, 
          "../s01e11_house_price/data/churn_test_2.csv", 
          col_names = TRUE) # Need this later to use with results

write_csv(churn_holdout, 
          "../s01e11_house_price/data/churn_holdout.csv", 
          col_names = FALSE) 
write_csv(id_holdout, 
          "../s01e11_house_price/data/churn_holdout_id.csv",
          col_names = FALSE) # Need this later to use with holdout results
```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}
xgb_spec <- 
  boost_tree(trees = 1000, 
             tree_depth = tune(),
             min_n = tune(),
             mtry = tune(),
             sample_size = tune(),
             learn_rate = tune()) %>%
  set_engine("xgboost") %>%
  set_mode("classification")

xgb_word_wf <- workflow(austin_rec, xgb_spec)

set.seed(123)
xgb_grid <-
  grid_max_entropy(
    tree_depth(c(5L, 10L)),
    min_n(c(10L, 40L)),
    mtry(c(5L, 10L)),
    sample_prop(c(0.5, 1.0)),
    learn_rate(c(-2, -1)),
    size = 20
  )

xgb_grid
```

Now we can tune across the grid of parameters and our resamples. Since we are trying
quite a lot of hyperparameter combinations, let's use
[racing](https://juliasilge.com/blog/baseball-racing/) to quit early on clearly bad
hyperparameter combinations.

```{r}
library(finetune)
doParallel::registerDoParallel()

set.seed(234)
xgb_word_rs <- 
  tune_race_anova(
    xgb_word_wf,
    austin_folds,
    grid = xgb_grid,
    metrics = metric_set(mn_log_loss),
    control = control_race(verbose_elim = TRUE)
  )

xgb_word_rs
```

That takes a little while but we did it!

## Evaluate results

First off, how did the "race" go?

```{r}
plot_race(xgb_word_rs)
```

We can look at the top results manually as well.

```{r}
show_best(xgb_word_rs)
```

Let's use `last_fit()` to fit one final time to the **training** data and evaluate
one final time on the **testing** data, with the numerically optimal result from
`xgb_word_rs`.

```{r}
xgb_last <- 
  xgb_word_wf %>%
  finalize_workflow(select_best(xgb_word_rs, "mn_log_loss")) %>%
  last_fit(austin_split)

xgb_last
```

How did this model perform on the testing data, that was not used in tuning/training?

```{r}
collect_predictions(xgb_last) %>%
    mn_log_loss(priceRange, `.pred_0-250000`:`.pred_650000+`)
```

This result is pretty good for a single (not ensembled) model and is a wee bit better
than what I did during the SLICED competition. I had an R bomb right as I was
finishing up tuning a model just like the one I am demonstrating here!

How does this model perform across the different classes?

```{r}
collect_predictions(xgb_last) %>%
    conf_mat(priceRange, .pred_class) %>%
    autoplot()
```

We can also visualize this with an ROC curve.

```{r}
collect_predictions(xgb_last) %>%
    roc_curve(priceRange,  `.pred_0-250000`:`.pred_650000+`) %>%
    ggplot(aes(1 - specificity, sensitivity, color = .level)) +
    geom_abline(lty = 2, color = "gray80", size = 1.5) +
    geom_path(alpha = 0.8, size = 1.2) +
    coord_equal() +
    labs(color = NULL)
```

Notice that it is easier to identify the most expensive homes but more difficult to
correctly classify the less expensive homes.

What features are most important for this xgboost model?

```{r}
library(vip)
extract_workflow(xgb_last) %>%
    extract_fit_parsnip() %>%
    vip(geom = "point", num_features = 15)
```

The spatial information in latitude/longitude are by far the most important. Notice
that the model uses `low_price_words` more than it uses, for example, whether there
is a spa or whether it is a single family home (as opposed to a townhome or condo).
It looks like the model is trying to distinguish some of those lower priced
categories. The model does *not* really use the `high_price_words` variable, perhaps
because it is already easy to find the expensive houses.

The two finalists from SLICED go on to compete next Tuesday, which should be fun and
interesting to watch! I have enjoyed the opportunity to participate this season.
