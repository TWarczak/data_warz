---
title: "Predicting Bank Customer Churn using AWS SageMaker and XGBoost in Local RStudio"
output: md_document
author: Todd Warczak
date: '2021-08-03'
slug: modeltime
categories:
   - rstats
   - AWS
   - SageMaker
   - XGBoost
   - R
tags:
   - rstats
   - AWS
   - reticulate
   - XGBoost
   - R
summary: "For this post, I experimented using AWS SageMaker with the AWS built-in XGBoost algorithm from within my local RStudio to predict whether a bank customer has churned. The data comes from the SLICED season 1 episode 7 Kaggle competition. SLICED is a data science competition where contestants are given a never-before-seen dataset and two-hours to code a solution to a prection challenge."
featured: "featured-hex-modeltime.jpeg"
image:
   caption: ''
   focal_point: ''
   preview_only: true
output: md_document
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
```

```{r library, echo=TRUE}
library(tidymodels)
library(tidyverse)
library(corrmorant)    # correlation matrix
library(patchwork)     # multiple plots to 1 plot
library(reticulate)    # calling the SageMaker Python SDK from R
library(pROC)          # ROC curves
library(viridis)       # color palletes
library(caret)         # 
```

-   [ü•Ö Project Goal](#goal)
-   [üóÇ Obtain Data](#data)
-   [üõÅ Clean Data](#clean)
-   [üî≠ Explore Data](#explore)
-   [üßÆ AWS Setup](#aws)
-   [üèó Train XGBoost](#build)
-   [üîß Tune XGBoost](#tune)
-   [üèÜ Select Best Model](#best)
-   [üéØ Predict Holdout](#holdout)
-   [üì¨ Submission](#submit)

## ü•Ö Goal of this Project {#goal}

Predict whether a bank customer will churn using AWS SageMaker and RStudio

## üóÇ Obtain Data {#data}

The data comes from the Kaggle competition
[SLICED](https://www.kaggle.com/c/sliced-s01e07-HmPsw2/overview).

```{r chunk3, message=FALSE, warning=FALSE}
churn_data <- read_csv("~/Documents/R/data_warz/content/post/2021-08-01-sagemaker-r-xgb-churn/train.csv")

holdout <- read_csv("~/Documents/R/data_warz/content/post/2021-08-01-sagemaker-r-xgb-churn/test.csv")
```

`holdout` does not contain `attrition_flag`. Use `holdout` to run inference after you've selected the best tuned model and built an endpoint to host the model. To submit predicted customer churn to the SLICED competition, we'll need a .csv file that contains only 2 columns: "id" (those found in `holdout`) and inferred `attrition_flag`. That will be our final step. 

## üõÅ Clean Data {#clean}

```{r chunk4, cache=TRUE}
glimpse(churn_data)
```

Let's look at the data dictionary to check we understand what each variable represents.  

### [Data Dictionary]

#### Outcome Variable

-   attrition_flag: whether the customer is churned (0 = no; 1 = yes)

#### ID

-   id: unique identifier for the customer

#### Categorical Features

-   gender
-   education_level
-   income_category: income range of the customer

#### Numeric Features

-   customer_age

-   total_relationship_count: number of relationships

-   months_inactive_12_mon: number of months the customer is inactive in past 12
    months

-   credit_limit

-   total_revolving_bal: customer's total revolving balance

-   total_amt_chng_q4_q1: amount the balance changed from Q4 to Q1

-   total_trans_amt: value of all the customer's transactions in the period

-   total_trans_ct: count of all the customer's transactions

-   total_ct_chng_q4_q1: difference in number of the customer's transactions from Q4
    to Q1

-   avg_utilization_ratio: customer's average utilization ratio during the period


I like using the `{skimr}` package to quickly check for missing values and get variable information.  

```{r chunk5, cache=TRUE}
skimr::skim_without_charts(churn_data)
```

Not much cleaning to do. For EDA I want the `attrition_flag` variable to be obvious whether the customer has churned or not. I'll factor the categorical variables and experiment with log transforming some numeric variables. 

##  üî≠ Explore Data {#explore}

```{r chunk6, message=FALSE, warning=FALSE}
churn_data2 <- churn_data %>% 
  mutate(churned = factor(ifelse(attrition_flag == 1, "yes", "no"),
                          levels = c("no", "yes"))) %>%
  mutate(education_level = fct_relevel(education_level, c("Unknown", 
                                                          "Uneducated", 
                                                          "High School", 
                                                          "College", 
                                                          "Graduate", 
                                                          "Doctorate", 
                                                          "Post-Graduate"))) %>%
  mutate(income_category = fct_relevel(income_category, c("Unknown",
                                                          "Less than $40K",
                                                          "$40K - $60K",
                                                          "$60K - $80K",
                                                          "$80K - $120K",
                                                          "$120K +")))

churn_data3 <- churn_data2 %>% 
               mutate(across(.cols = c(credit_limit, total_trans_amt),
                             .fns  = log1p)) %>%
               mutate(across(.cols = c(credit_limit, total_trans_amt), 
                             .fns = timetk::standardize_vec)) %>% 
               select(-c(id, attrition_flag))
```

Let's look at a correlation matrix of all the numeric variables. We can group by our new `churned` variable to see features that correlate differently based on the churn outcome.  

```{r chunk7, message=FALSE, warning=FALSE, cache=TRUE}

corfun <- function(x, y)  {
  round(cor(x, y, method = "pearson", use = "pairwise.complete.obs"), 2)
}

ggcorrm(churn_data3, aes(col = churned, fill = churned), bg_dia  = "grey30") +
    lotri(geom_point(alpha = 0.1)) +
    lotri(geom_smooth(se=F, method = "loess")) +
    utri_funtext(fun = corfun, size = 6) +
    dia_names(y_pos = 0.15, size = 3) +
    dia_density(lower = 0.3, fill = "grey60", color = 1) +
    theme_dark() +
    scale_color_viridis_d() +
    scale_fill_viridis_d() +
    labs(title = "Correlation Plot")
```
It looks to me that `total_ct_chng_q4_q1`, `total_trans_amt`, `total_trans_ct`, `credit_limit`, and `avg_utilization_ratio` might be important to churn. I want to see how the numeric feature density plots look and see if we can see any distributions that are clearly separated by churn.  

```{r chunk8, cache=TRUE}
dense <- churn_data3 %>%
         select(-c(gender, education_level, income_category)) %>%
         pivot_longer(cols = c(customer_age:avg_utilization_ratio), 
                      names_to = "feature", 
                      values_to = "value")

ggplot(dense, aes(value, fill = churned)) +
  geom_density(alpha = .5) +
  facet_wrap(~ feature, scales = "free") +
  labs(title = "Numeric features impacting churn?")
```
We see some clues in the distributions of `total_trans_amt`, `total_trans_ct`, `total_relationship_count`, and `total_revolving_bal`. `months_inactive_12_mon` might be important, but mostly when the variable = 1 month.  

Let's try and find some combinations of these features that lead to churn more often. Last data viz for the numeric features.

```{r chunk9.1, cache=TRUE}
churn_data %>% 
  ggplot(aes(total_trans_amt, total_trans_ct, z = attrition_flag)) +
  stat_summary_hex(alpha = 0.8, bins = 50) +
  scale_fill_viridis_c() +
  labs(fill = "% Churned") +
  theme_dark()
```
```{r chunk9.2, cache=TRUE}
churn_data %>% 
  ggplot(aes(total_ct_chng_q4_q1, total_trans_ct, z = attrition_flag)) +
  stat_summary_hex(alpha = 0.8, bins = 50) +
  scale_fill_viridis_c() +
  labs(fill = "% Churned") +
  theme_dark()
```
```{r chunk9.3, cache=TRUE}
churn_data %>% 
  ggplot(aes(credit_limit, avg_utilization_ratio, z = attrition_flag)) +
  stat_summary_hex(alpha = 0.8, bins = 50) +
  scale_fill_viridis_c() +
  labs(fill = "% Churned") +
  theme_dark() 
```
```{r chunk9.4, cache=TRUE}
churn_data %>% 
  ggplot(aes(total_trans_amt, total_amt_chng_q4_q1, z = attrition_flag)) +
  stat_summary_hex(alpha = 0.8, bins = 60) +
  scale_fill_viridis_c() +
  labs(fill = "% Churned") +
  theme_dark()
```
It's clear that churn will be predicted if a customer data contains certain combination of feature values (yellow). 

Now let's get a count for each categorical variable and a percentage churn for each category to see how balanced the dataset is. 
```{r chunk10, echo=FALSE, cache=TRUE}
cat_feat <- churn_data2 %>% 
  select(c(gender, education_level, income_category, churned))

n_gen <- cat_feat %>% 
  group_by(gender) %>% 
  summarise(count = n()) %>% 
  ggplot(aes(gender, count)) +
  geom_col() +
  coord_flip() +
  geom_text(aes(label = count),
            color    = "white",
            size     = 3,
            fontface = 'bold',
            hjust    = 1.2,
            vjust    = 0.4) +
        theme_minimal() +
        theme(legend.position = 'none',
              axis.title.x = element_blank())

n_edu <- cat_feat %>% 
  group_by(education_level) %>% 
  summarise(count = n()) %>% 
  ggplot(aes(education_level, count)) +
  geom_col() +
  coord_flip() +
  geom_text(aes(label = count),
            color    = "white",
            size     = 3,
            fontface = 'bold',
            hjust    = 1.2,
            vjust    = 0.4) +
        theme_minimal() +
        theme(legend.position = 'none')

n_inc <- cat_feat %>% 
  group_by(income_category) %>% 
  summarise(count = n()) %>% 
  ggplot(aes(income_category, count)) +
  geom_col() +
  coord_flip() +
  geom_text(aes(label = count),
            color    = "white",
            size     = 3,
            fontface = 'bold',
            hjust    = 1.2,
            vjust    = 0.4) +
        theme_minimal() +
        theme(legend.position = 'none')

n_chrn <- cat_feat %>% 
  group_by(churned) %>% 
  summarise(count = n()) %>% 
  ggplot(aes(churned, count)) +
  geom_col() +
  coord_flip() +
  geom_text(aes(label = count),
            color    = "white",
            size     = 3,
            fontface = 'bold',
            hjust    = 1.2,
            vjust    = 0.4) +
        theme_minimal() +
        theme(legend.position = 'none',
              axis.title.x = element_blank())

n_chrn + n_gen + n_edu + n_inc +
plot_layout(ncol = 2, heights = c(0.7,2)) +
plot_annotation(title = "Counts of Each Categorical Variable in Training Data")
```
Our churn variable is unbalanced. Would be better if we had more data points for those that have churned. We could upsample positive churn outcome or downsample the negative churn outcome, but I'll let it slide this time.  
How balanced are the categorical predictors?

```{r chunk11, echo=FALSE, cache=TRUE}
gen <- ggplot(cat_feat %>% 
                group_by(gender, churned) %>% 
                summarise(n = n()) %>% 
                mutate(percent_churned = prop.table(n)*100) %>% 
                filter(churned == 'yes'),
               aes(gender, percent_churned)) +
        geom_col() +
        coord_flip() +
        geom_text(aes(label = round(percent_churned, digits = 1)),
                  color    = "white",
                  size     = 4,
                  fontface = 'bold',
                  hjust    = 1.3,
                  vjust    = 0.4) +
        theme_minimal() +
        theme(legend.position = 'none')

ed <- ggplot(cat_feat %>% 
                group_by(education_level, churned) %>% 
                summarise(n = n()) %>% 
                mutate(percent_churned = prop.table(n)*100) %>% 
                filter(churned == 'yes'),
             aes(education_level, percent_churned)) +
        geom_col() +
        coord_flip() +
        geom_text(aes(label = round(percent_churned, digits = 1)),
                    color    = "white",
                    size     = 4,
                    fontface = 'bold',
                    hjust    = 1.3,
                    vjust    = 0.4) +
        theme_minimal() +
        theme(axis.title.x = element_blank(),
              legend.position = 'none')

inc <- ggplot(cat_feat %>% 
                group_by(income_category, churned) %>% 
                summarise(n = n()) %>% 
                mutate(percent_churned = prop.table(n)*100) %>% 
                filter(churned == 'yes'),
              aes(income_category, percent_churned)) +
        geom_col() +
        coord_flip() +
        geom_text(aes(label = round(percent_churned, digits = 1)),
                    color    = "white",
                    size     = 4,
                    fontface = 'bold',
                    hjust    = 1.3,
                    vjust    = 0.4) +
        theme_minimal() +
        theme(axis.title.x = element_blank(),
              legend.position = 'none')

ed + inc + gen +
    plot_layout(ncol = 2, heights = c(2,0.7)) +
    plot_annotation(title = "Percentage of Each Categorical Variable that Churned")

```

```{r chunk12, echo=FALSE, cache=TRUE}
cat_feat %>%
  pivot_longer(gender:income_category) %>%
  ggplot(aes(y = value, fill = churned)) +
  geom_bar(position = "fill") +
  facet_wrap(vars(name), scales = "free", ncol = 1) +
  theme_minimal() +
  labs(x = NULL, y = NULL)
```
At least the data has consistantly low churn (12%-21%) across all categorical values. I'd guess the numeric features are more important based on what we've seen. 

Now it's time to model. You need to set up your own AWS account and get RStudio connected. Alex Lemm has a great repo that guides you through configuring RStudio to work with AWS tools [github.com/alex23lemm](https://github.com/alex23lemm/AWS-SageMaker-Fundamentals-R-Workshop). You'll find a more detailed explaination of my code chunks within his projects if you want a deeper understanding.

We need to set up our AWS SageMaker session using a conda or miniconda environment and the `{reticulate}` package. 

This code works on the AWS 'Free Tier' btw. 

```{r chunk13}
use_condaenv("sagemaker-r", required = TRUE)
sagemaker <- import("sagemaker")
session <- sagemaker$Session()
```

Select the S3 bucket you want to upload your data and models to. Then create a `data` and `models` folder.

```{r chunk14}
# bucket <- session$default_bucket()
bucket <- "twarczak-sagemaker3"
project <- "churn"
data_path <- paste0("s3://", bucket, "/", project, "/", "data")
models_path <- paste0("s3://", bucket, "/", project, "/", "models")
```

### Data splitting

Split the `churn_data` into a train/validation/test. We will use the SageMaker built-in XGBoost algorithm and Bayesian Optimation for hyperparameter tuning. Info about this version of XGBoost, hyperparameters, metrics, etc. can be found here  [AWS_XGBoost](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html). 

I am also using the original data, not the dataframes I manipulated for our EDA.  
```{r chunk15, cache=TRUE}
# Make 70/15/15 Train/Validate/Test split

set.seed(333)
splits  <- initial_split(churn_data, prop = 0.70, strata = attrition_flag)

train <- training(splits)
other  <- testing(splits)

splits2 <- initial_split(other, prop = 0.50, strata = attrition_flag)
validation <- training(splits2)
test <- testing(splits2)

print(splits)
print(splits2)
```
### Preprocessing the data

Let's make a simple recipe for our data. XGBoost doesn't need numeric variable transformations, but we need to one-hot encode the categorical features. SageMaker XGBoost doesn't want id variables and the target (`attrition_flag`) must be the first column. We'll also remove the table header later. 

```{r chunk16, cache=TRUE}
churn_rec <- recipe(attrition_flag ~ ., data = train) %>% 
             step_dummy(all_nominal_predictors(), one_hot = TRUE) %>% 
             step_rm(id) %>% 
             prep()

churn_training <- churn_rec %>% 
                  juice() %>% 
                  select(attrition_flag, everything()) # just to put attrition_flag as 1st column

churn_validation <- bake(churn_rec, new_data = validation) %>% 
                    select(attrition_flag, everything()) # just to put attrition_flag as 1st column

churn_test <- bake(churn_rec, new_data = test) %>% 
              select(attrition_flag, everything()) # just to put attrition_flag as 1st column

id_holdout <- holdout %>% select(id) 
churn_holdout <- bake(churn_rec, new_data = holdout)
```

Great. Now all our data is transformed and ready for SageMaker XGBoost.  First though, let's run the training data through a basic XGBoost algo in R, just to look at feature importance.

```{r chunk17, message=FALSE, warning=FALSE, cache=TRUE}
bst <- xgboost::xgboost(data = data.matrix(subset(churn_training, select = -attrition_flag)), 
               label = churn_training$attrition_flag, max_depth = 2, eta = 0.5, nthread = 2, nrounds = 100, 
               objective = "binary:logistic", verbose = FALSE)

imp_bst <- xgb.importance(model = bst)
xgb.ggplot.importance(imp_bst)
```
```{r chunk17.5, include=FALSE}
vip::vip(bst, geom = 'point', num_features = 15)
```
We should have looked harder at `total_revolving_balance`, but this mostly agrees with our EDA assessment. 

Save the pre-processed data. The test set must lack the target variable.  Save holdout data id's to join to the predicted values at the very end.  
```{r chunk18, message=FALSE}

dir.create("../2021-08-01-sagemaker-r-xgb-churn/data")

write_csv(churn_training, 
          "../2021-08-01-sagemaker-r-xgb-churn/data/churn_training.csv", 
          col_names = FALSE)
write_csv(churn_validation, 
          "../2021-08-01-sagemaker-r-xgb-churn/data/churn_validation.csv", 
          col_names = FALSE)
write_csv(churn_test %>% select(-attrition_flag), 
          "../2021-08-01-sagemaker-r-xgb-churn/data/churn_test.csv", 
          col_names = FALSE)
write_csv(churn_test, 
          "../2021-08-01-sagemaker-r-xgb-churn/data/churn_test_2.csv", 
          col_names = TRUE) # Need this later to use with results


write_csv(churn_holdout, 
          "../2021-08-01-sagemaker-r-xgb-churn/data/churn_holdout.csv", 
          col_names = FALSE) 
write_csv(id_holdout, 
          "../2021-08-01-sagemaker-r-xgb-churn/data/churn_holdout_id.csv",
          col_names = FALSE) # Need this later to use with holdout results
```


```{r chunk19, cache=TRUE}
s3_uploader <- sagemaker$s3$S3Uploader()

s3_train <- s3_uploader$upload(local_path = "../2021-08-01-sagemaker-r-xgb-churn/data/churn_training.csv", 
                               desired_s3_uri = data_path)

s3_validation <- s3_uploader$upload(local_path = "../2021-08-01-sagemaker-r-xgb-churn/data/churn_validation.csv",
                                    desired_s3_uri = data_path)

s3_test <- s3_uploader$upload(local_path = "../2021-08-01-sagemaker-r-xgb-churn/data/churn_test.csv", 
                              desired_s3_uri = data_path)


```

```{r chunk20, cache=TRUE}
region <- session$boto_region_name
# get container image location
container <- sagemaker$image_uris$retrieve(framework = "xgboost", 
                                           region    = region, 
                                           version   = "1.3-1" )
# get SageMaker execution role stored in .Renviron
role_arn <- Sys.getenv("SAGEMAKER_ROLE_ARN")
# Create an Estimator object
xgb_estimator <- sagemaker$estimator$Estimator(image_uri          = container,
                                               role               = role_arn,
                                               instance_count     = 1L,
                                               instance_type      = "ml.m5.4xlarge",
                                               volume_size        = 10L, #30L
                                               max_run            = 300L,
                                               output_path        = models_path,
                                               sagemaker_session  = session,
                                               use_spot_instances = TRUE,
                                               max_wait           = 300L )
```

```{r chunk21, cache=TRUE}
xgb_estimator$set_hyperparameters(objective        = "binary:logistic",
                                  eval_metric      = "auc",
                                  max_depth        = 5L, 
                                  eta              = 0.1,
                                  num_round        = 100L,
                                  colsample_bytree = 0.4,
                                  alpha            = 10L,
                                  min_child_weight = 1.1,
                                  subsample        = 0.7)
```

```{r chunk22, cache=TRUE}
# Create training job name based project organization principles
algo <- "xgb"
timestamp <- format(Sys.time(), "%Y-%m-%d-%H-%M-%S")
job_name <- paste(project, algo, timestamp, sep = "-")
s3_train_input <- sagemaker$inputs$TrainingInput(s3_data = s3_train,
                                                 content_type = 'csv')
s3_valid_input <- sagemaker$inputs$TrainingInput(s3_data = s3_validation,
                                                 content_type = 'csv')
input_data <- list('train'      = s3_train_input,
                   'validation' = s3_valid_input)
```

```{r chunk23, cache=TRUE}
xgb_estimator$fit(inputs = input_data,
                  job_name = job_name,
                  wait = FALSE)  # If set to TRUE, the call will wait until job completes
```

```{r chunk24, cache=TRUE}
session$describe_training_job(job_name)[["TrainingJobStatus"]]
```

```{r chunk25, cache=TRUE}
training_job_stats <- session$describe_training_job(job_name = job_name)
final_metrics <-  map_df(training_job_stats$FinalMetricDataList, 
                          ~tibble(metric_name = .x[["MetricName"]],
                                  value = .x[["Value"]]))
final_metrics
```

```{r chunk26, cache=TRUE}
predictions_path <- paste0(models_path, "/", job_name, "/predictions") 
xgb_batch_predictor <- xgb_estimator$transformer(instance_count = 1L, 
                                                 instance_type  = "ml.m5.xlarge", 
                                                 strategy       = "MultiRecord",
                                                 assemble_with  = "Line",
                                                 output_path    = predictions_path)
```

```{r chunk27, cache=TRUE}
xgb_batch_predictor$transform(data         = s3_test, 
                              content_type = 'text/csv',
                              split_type   = "Line",
                              job_name     = job_name,
                              wait         = FALSE) # If TRUE, call waits until job completes
```

```{r chunk28, cache=TRUE}
session$describe_transform_job(job_name)[["TransformJobStatus"]]
```

```{r chunk29, cache=TRUE}
s3_downloader <- sagemaker$s3$S3Downloader()
s3_test_predictions_path <- s3_downloader$list(predictions_path)
 
dir.create("./predictions")
s3_downloader$download( s3_test_predictions_path, "./predictions")
 
test_predictions <- read_csv("./predictions/churn_test.csv.out",
                              col_names = FALSE) %>% 
                    pull(X1)

test_results <- tibble(truth       = churn_test$attrition_flag,
                       predictions = test_predictions )

head(test_results)
```

```{r chunk30, cache=TRUE}
roc_obj <- roc(test_results$truth,
               test_results$predictions,
               plot        = TRUE,         
               grid        = TRUE,
               print.auc   = TRUE,
               legacy.axes = TRUE, 
               main        = "ROC curve for XGBoost classification",
               show.thres  = TRUE,
               col         = "red2" )
```

```{r chunk31, cache=TRUE}
conf_matrix <- caret::confusionMatrix(factor(ifelse(test_results$predictions >= 0.5, 1, 0),
                                             levels = c("0", "1"), 
                                             labels = c("retained", "churned")),
                                      factor(test_results$truth, 
                                             levels = c(0, 1), 
                                             labels = c("retained", "churned")),
                                      positive = "churned")

conf_matrix
```

```{r chunk32}
xgb_estimator$set_hyperparameters(objective        = "binary:logistic",
                                  min_child_weight = 1 )
```

```{r chunk33}
hyperp_ranges <- list(num_round = sagemaker$tuner$IntegerParameter(50L, 500L),
                      max_depth = sagemaker$tuner$IntegerParameter(2L, 15L),
                      eta =sagemaker$tuner$ContinuousParameter(0.01,0.4,"Logarithmic"),
                      colsample_bytree = sagemaker$tuner$ContinuousParameter(0.3,0.8),
                      alpha = sagemaker$tuner$IntegerParameter(1L, 15L))
```

```{r chunk34}
tuner <- sagemaker$tuner$HyperparameterTuner(estimator             = xgb_estimator,
                                             objective_metric_name = "validation:auc",
                                             objective_type        = "Maximize",
                                             hyperparameter_ranges = hyperp_ranges, 
                                             strategy              = "Bayesian",
                                             max_jobs              = 50L,
                                             max_parallel_jobs     = 2L )

```

```{r chunk35}

algo <- "xgb"
timestamp <- format(Sys.time(), "%Y-%m-%d-%H-%M-%S")
job_name <- paste(project, algo, timestamp, sep = "-")
s3_train_input <- sagemaker$inputs$TrainingInput(s3_data = s3_train,
                                                 content_type = 'csv')
s3_valid_input <- sagemaker$inputs$TrainingInput(s3_data = s3_validation,
                                                 content_type = 'csv')
input_data <- list('train'      = s3_train_input,
                   'validation' = s3_valid_input)
```

```{r chunk36}
tuner$fit(inputs   = input_data, 
          job_name = job_name,
          wait     = FALSE ) # If TRUE, call will wait until job completes
```


```{r chunk37, cache=TRUE}
session$describe_tuning_job(job_name)[["HyperParameterTuningJobStatus"]]
```


```{r chunk38, cache=TRUE}
tuning_job_results <- sagemaker$HyperparameterTuningJobAnalytics(job_name)
tuning_results_df <- tuning_job_results$dataframe()
head(tuning_results_df)#[, c(1:4, 6)]

```


```{r chunk39, fig.align='center', cache=TRUE}
ggplot(tuning_results_df, aes(TrainingEndTime, FinalObjectiveValue)) +
  geom_point() +
  xlab("Time") +
  ylab(tuning_job_results$description()$TrainingJobDefinition$StaticHyperParameters$`_tuning_objective_metric`) +
  ggtitle("Hyperparameter tuning objective metric",  
          "Progression over the period of all 50 training jobs") +
  theme_minimal()
```


```{r chunk40, fig.align='center', cache=TRUE}
ggplot(tuning_results_df, aes(num_round, eta)) +
  geom_point(aes(color = FinalObjectiveValue)) +
  scale_color_viridis("AUC", option = "H") +
  ggtitle("Hyperparameter tuning objective metric progression", 
          "Using a Bayesian strategy") +
  theme_dark()
```


```{r chunk41, cache=TRUE}
best_tuned_model <- tuning_results_df %>%
                    filter(FinalObjectiveValue == max(FinalObjectiveValue)) %>%
                    filter(alpha == 1) %>%   # there were 2 jobs with max value
                    pull(TrainingJobName)

best_tuned_model

training_job_stats <- session$describe_training_job(job_name = best_tuned_model)

final_metrics <-  map_df(training_job_stats$FinalMetricDataList, 
                          ~tibble(metric_name = .x[["MetricName"]],
                                  value       = .x[["Value"]]))
final_metrics
```


```{r chunk42}
predictions_path <- paste0(models_path, "/", best_tuned_model, "/predictions")

session$create_model_from_job(best_tuned_model)

xgb_batch_predictor <- sagemaker$transformer$Transformer(model_name     = best_tuned_model,
                                                         instance_count = 1L, 
                                                         instance_type  = "ml.m5.4xlarge", 
                                                         strategy       = "MultiRecord",
                                                         assemble_with  = "Line",
                                                         output_path    = predictions_path)

```


```{r chunk43}
xgb_batch_predictor$transform(data         = s3_test, 
                              content_type = 'text/csv',
                              split_type   = "Line",
                              job_name     = best_tuned_model,
                              wait         = FALSE  ) # If TRUE, call will wait until job completes

```

```{r chunk44, cache=TRUE}
session$describe_transform_job(best_tuned_model)[["TransformJobStatus"]]
```
```{r chunk45, cache=TRUE}
s3_downloader <- sagemaker$s3$S3Downloader()

s3_test_predictions_path <- s3_downloader$list(predictions_path)
 
dir.create("./predictions")

s3_downloader$download(s3_test_predictions_path, "./predictions")
 
test_predictions <- read_csv("./predictions/churn_test.csv.out",
                              col_names = FALSE) %>% 
                    pull(X1)

churn_test <- read_csv("./data/churn_test_2.csv")

test_results <- tibble(truth       = churn_test$attrition_flag,
                       predictions = test_predictions)

head(test_results)
```

```{r chunk46, cache=TRUE}
roc_obj <- roc(test_results$truth, 
               test_results$predictions,
               plot        = TRUE,         
               grid        = TRUE,
               print.auc   = TRUE,
               legacy.axes = TRUE, 
               main        = "ROC curve for XGBoost classification using the best model",
               show.thres  = TRUE,
               col         = "red2" )
```

Creating a confusion matrix using the `caret` package we see the following results:

```{r chunk47, cache=TRUE}
conf_matrix <- confusionMatrix(factor(ifelse(test_results$predictions >= 0.5, 1, 0),
                                      levels = c("0", "1"), 
                                      labels = c("retained", "churned")),
                               factor(test_results$truth, 
                                      levels = c(0, 1), 
                                      labels = c("retained", "churned")),                               
                                positive = "churned")

conf_matrix
```

```{r chunk48, cache=TRUE}
test_results_pred <- test_results %>% 
                     mutate(pred = factor(ifelse(predictions >= 0.5, 1, 0), 
                                          levels = c("0", "1")))

gmodels::CrossTable(x          = test_results_pred$pred, 
                    y          = test_results_pred$truth,
                    prop.chisq = FALSE, 
                    prop.r     = FALSE, 
                    prop.c     = FALSE, 
                    prop.t     = FALSE,
                    dnn        = c('predicted churn', 'actual churn'))
```

```{r chunk49, cache=TRUE}
boto_client <- session$boto_session$client("sagemaker")
churn_model <- boto_client$list_models()[["Models"]] %>% 
  map_chr("ModelName") %>% 
  .[[1]]

churn_model
```


```{r chunk50, results='hide'}
config_name <- paste0(churn_model, "-config")
session$create_endpoint_config(name = config_name,
                               model_name =  churn_model, 
                               initial_instance_count = 1L, 
                               instance_type = "ml.m5.large")
```


```{r chunk51, results='hide'}
endpoint_name <- "churn-endpoint"
session$create_endpoint(endpoint_name = endpoint_name, 
                        config_name = config_name,
                        wait = FALSE)
```

```{r chunk52, cache=TRUE}
boto_client$describe_endpoint(EndpointName = endpoint_name)[["EndpointStatus"]]
```

```{r chunk53, cache=TRUE}
# Instantiate CSV-Serializer/CSV-Deserializer objects
csv_serializer <- sagemaker$serializers$CSVSerializer()

# csv_serializer$CONTENT_TYPE <- "text/csv" # this is the default
csv_deserializer <- sagemaker$deserializers$CSVDeserializer()

# Instantiate Predictor object
churn_predictor <- sagemaker$predictor$Predictor(endpoint_name     = endpoint_name, 
                                                 sagemaker_session = session, 
                                                 serializer        = csv_serializer,
                                                 deserializer      = csv_deserializer)
```


```{r chunk54, message=FALSE, cache=TRUE}
test_set <- read_csv("./data/churn_test.csv", 
                     col_names = FALSE, 
                     n_max     = 5) %>% 
            as.matrix()
real_time_predictions <- churn_predictor$predict(data = test_set) %>% 
                         .[[1]] %>% 
                         as.numeric()
```

```{r chunk55, message=FALSE, cache=TRUE}
batch_predictions <- read_csv("./predictions/churn_test.csv.out", 
                              col_names = FALSE, n_max = 5) %>% 
                     .[[1]]
  
data.frame(real_time_predictions, batch_predictions)
```


```{r chunk56, message=FALSE, cache=TRUE}
holdout_set <- read_csv("./data/churn_holdout.csv", 
                     col_names = FALSE) %>% 
               as.matrix()
real_time_predictions <- churn_predictor$predict(data = holdout_set) %>% 
                         .[[1]] %>% 
                         as.numeric()
```

```{r chunk57, cache=TRUE }
churn_holdout_id <- read_csv("data/churn_holdout_id.csv", col_names = FALSE)
submission <- data.frame(churn_holdout_id, real_time_predictions ) %>% 
              rename(id = X1, attrition_flag = real_time_predictions)
write.csv(submission, "sliced_s01_e07_submission.csv", row.names = FALSE)


```


```{r chunk58}
#session$delete_endpoint(endpoint_name)
```
