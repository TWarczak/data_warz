---
title: Forecasting Daily Sales w/ {modeltime}
subtitle: Time-Series Modeling in the {tidymodels} Universe
author: Todd Warczak
date: '2021-05-20'
categories: [R]
description: "In my introductory blog post, I use the time-series modeling package {modeltime} and the Superstore Sales Dataset found on Kaggle to forecast 3 months of daily sales.  After some initial EDA I compare how traditional time-series models like ARIMA and more newer models like Prophet-XGboost and NNETAR forecast on the dataset.  This is a {tidymodels} approach with with a {modeltime} twist."
tags: ["tidymodels", "modeltime", "time-series", "tidyverse", "R", "rstats"]
output:
   html_document:
      code_folding: show
editor_options:
  chunk_output_type: inline

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, fig.showtext = TRUE, retina = 2)
```

```{r, include=FALSE}
library(tidyverse)
library(tidymodels)
library(modeltime)
library(modeltime.ensemble)
library(modeltime.resample)
library(timetk)
library(lubridate)
```

```{r, include=FALSE}
theme_dark_grey <- function(base_size=14, base_family="sans") {
   library(grid)
   library(ggthemes)
   (theme_foundation(base_size=base_size, base_family=base_family)
      + theme(plot.title = element_text(face = "bold", colour = '#ffffb3',
                                        size = rel(1.1), hjust = 0.0, margin = margin(0,0,5,0)),
              text = element_text(),
              panel.background = element_rect(colour = NA, fill = 'grey20'),
              plot.background = element_rect(colour = NA, fill = '#262626'),
              panel.border = element_rect(colour = NA),
              axis.title = element_text(face = "bold",size = rel(1), colour = 'white'),
              axis.title.y = element_text(angle=90,vjust =2),
              axis.title.x = element_text(vjust = -0.2),
              axis.text = element_text(colour = 'grey70'), 
              axis.line.x = element_line(colour="grey70"),
              axis.line.y = element_line(colour="grey70"),
              axis.ticks = element_line(colour="grey70"),
              panel.grid.major = element_line(colour="#262626"),
              panel.grid.minor = element_blank(),
              legend.background = element_rect(fill ='#262626'),
              legend.text = element_text(color = 'white'),
              legend.key = element_rect(colour = NA, fill = '#262626'),
              legend.position = "bottom",
              legend.direction = "horizontal",
              legend.box = "vetical",
              legend.key.size= unit(0.5, "cm"),
              #legend.margin = unit(0, "cm"),
              legend.title = element_text(face="italic", colour = 'white'),
              plot.margin=unit(c(5,5,5,5),"mm"),
              strip.background=element_rect(colour="#2D3A4C",fill="#2D3A4C"),
              strip.text = element_text(face="bold", colour = 'white')
      ))
}

scale_fill_todd_dark <- function(...){
   library(scales)
   discrete_scale("fill","todd",manual_pal(values = c("#decbe4","#fed9a6","#b3cde3","#ccebc5","#fbb4ae",
                                                      "#ffffcc","#e5d8bd","#fddaec","#f2f2f2")), ...)
}

scale_color_todd_dark_subtle <- function(...){
   library(scales)
   discrete_scale("color","todd",manual_pal(values = c("#decbe4","#fed9a6","#b3cde3","#ccebc5","#fbb4ae",
                                                       "#ffffcc","#e5d8bd","#fddaec","#f2f2f2")), ...)
}

scale_color_todd_dark_bright <- function(...){
   library(scales)
   discrete_scale("color","todd",manual_pal(values = c("#999999","#d708fb","#ffffcc","#75e6da","#94c973",
                                                       "#d4f1f4","#d3ae7c","#307ca1","#ffdf00","#f2f2f2")), ...)
}
```
![](/Users/toddwarczak/Documents/R/data_warz/content/post/2021-05-20-forecasting-daily-sales-w-modeltime/logo-modeltime.png)


* [ü•Ö Project Goal](#goal)
* [üóÇ Obtain Data](#data)
* [üõÅ Clean Data](#clean)
* [üî≠ Explore Data](#explore)
* [üßÆ Prep Data for Forecasting](#prep)
* [üèó Build Workflows](#build)
* [üîß Tune Models](#tune)
* [üèÜ Forecast w/ Best Models](#forecast)
* [üèÅ Ensemble & Save Work](#ensemble)


## ü•Ö Goal of this Project {#goal}

For this blog post, I want to showcase the `{modeltime}` packages to forecast 3 months of daily sales for a young 'Superstore' company selling furniture, technology, and office supplies.      


## üóÇ Obtain Data {#data}

The [Superstore Sales Dataset](https://www.kaggle.com/rohitsahoo/sales-forecasting/) on Kaggle is relatively unexplored.  There's a couple python exploratory data analysis projects posted but nothing using R and only one 7-day forecast using a simple SARIMAX model. Hopefully we can blow that one forecast out of the water using mainly `{timetk}` and `{modeltime}`.       
```{r data, cache=TRUE}
df <- read_csv("train.csv")
```

## üõÅ Clean Data {#clean}
```{r}
glimpse(df)
```

After a little digging a few things become obvious. There are two date variables, but lets only use `Order Date` going forward. It seems `Order Date` format is day/month/year. They only ship within the United States. We don't need individual `order_id` numbers. Let's clean up column names, format the date, factor the categorical variables, and remove what we definitely don't need.   
```{r}
data_clean_tbl <- janitor::clean_names(df) %>% 
                  as_tibble() %>% 
                  mutate(order_date = lubridate::dmy(order_date),
                         across(customer_id:product_name, .fns = as.factor)) %>% 
                  select(-c(ship_date, country, order_id, ship_mode))
glimpse(data_clean_tbl)
```

What period of time does this data cover?
```{r}
t <- summarise_by_time(.data     = data_clean_tbl,
                       .date_var = order_date)
(diff_t <- difftime(last(t$order_date),
                    first(t$order_date),
                    units = 'weeks'))
lubridate::dweeks(208.1429)
```

This is a daily dataset spanning just under 4 years from 2015-01-03 to 2018-12-30. There are gaps though!  


## üî≠ Exploratory Data Analysis {#explore}

Let's group by `region`/`state` to see where the 'Superstore' does business, looking at total `orders` and total `sales`.  
```{r}
q <- data_clean_tbl %>%
     group_by(region,state) %>%
     summarise(orders = n(), sales = sum(sales)) %>%
     ungroup() %>%
     mutate(state = tidytext::reorder_within(state, orders, region)) %>%
     arrange(desc(sales))

```
```{r Orders by State/Region, fig.width = 18.5, fig.height = 12.5, cache=TRUE, include=FALSE}

ggplot(q, aes(reorder_within(state, orders, region, sep = ""), orders, fill = region)) +
   geom_col(show.legend = FALSE, alpha       = 0.9) +
   facet_grid(region ~., scales = 'free', space = 'free') +
   coord_flip() +
   tidytext::scale_x_reordered() +  # gets rid of the ___region
   scale_y_continuous(limits = c(0,2000), expand = c(0,0)) +
   theme_dark_grey() +
   scale_color_todd_dark_subtle() +
   scale_fill_todd_dark() +
   theme(panel.grid.major.y = element_blank(),
         panel.grid.major.x = element_blank()) +
   geom_hline(yintercept = c(500,1000,1500),
              color      = "#FFCF6A",
              alpha      = 0.1,
              size       = 1) +
   labs(x     = element_blank(),
        title = 'Orders by State/Region') +
   geom_text(aes(label = orders),
             color    = "white",
             size     = 3.5,
             fontface = 'bold',
             hjust    = -0.2,
             vjust    = 0.4)

```
```{r Sales by State/Region, fig.width = 18.5, fig.height = 12.5, cache=TRUE, class.source = 'fold-hide'}

ggplot(q, aes(reorder_within(state, sales, region, sep = ""), sales, fill = region)) +
   geom_col(show.legend = FALSE, alpha = 0.9) +
   facet_grid(region ~., scales = 'free', space = 'free') +
   coord_flip() +
   tidytext::scale_x_reordered() +  # gets rid of the ___region
   scale_y_continuous(limits = c(0,500000), expand = c(0,0), labels=dollar_format(prefix="$")) +
   theme_dark_grey() +
   scale_color_todd_dark_subtle() +
   scale_fill_todd_dark() +
   theme(panel.grid.major.y = element_blank(),
         panel.grid.major.x = element_blank()) +
   geom_hline(yintercept = c(100000,200000,300000,400000),
              color      = "#FFCF6A",
              alpha      = 0.1,
              size       = 1) +
   labs(x = element_blank(), title = 'Sales by State/Region') +
   geom_text(aes(label = dollar(round((sales)))),
             color="white", size=3.5, fontface = 'bold',
             hjust = -0.1, vjust = 0.4)

```

Looks like the do business throughout the US, with the most business coming from California. They probably do most of their business from online shopping, as these total sale numbers aren't high enough for multiple warehouses spread across the country.  

Lets also look at number of `orders` and `sales` grouped by `customer_name`, `category`, and `segment`. Any outliers?
```{r, warning=FALSE, message=FALSE, cache=TRUE, class.source = 'fold-hide'}
# Want to identify any outlier customers, and see if any category/segment of their business dominates
x <- data_clean_tbl %>%
      group_by(customer_name, category, segment) %>%
      summarise(sales  = sum(sales),
                orders = n()) %>%
      mutate(log_sales = log(sales))

z <- data_clean_tbl %>%
      group_by(category, segment) %>%
      summarise(sales  = sum(sales),
                orders = n()) %>%
      ungroup() %>%
      mutate(orders_pct      = round(prop.table(orders) * 100),
             sales_pct       = round(prop.table(sales) * 100),
             sales_per_order = round(sales/orders)) %>%
      pivot_longer(orders_pct:sales_pct,
                   names_to  = 'metric',
                   values_to = 'percent')

outlier_furniture_consumer <- x %>% filter(segment  == 'Consumer',
                                            category == "Furniture") %>%
                                     arrange(desc(orders)) %>% head()

outlier_technology_corporate_homeoffice <- x %>%
                                           filter(segment  == 'Corporate' | segment == "Home Office",
                                                  category == "Technology") %>%
                                           arrange(desc(log_sales)) %>% head()
# Make specific tbl for outlier Seth Vernon so label doesn't appear in other facets
seth_vernon <- data.frame(category = factor("Furniture", levels = c("Furniture","Office Supplies","Technology")),
                          segment  = factor("Consumer", levels = c("Consumer","Corporate","Home Office")))

```

```{r Orders & Sales in log($) per Customer by Category/Segment, fig.width = 16, fig.height = 10, cache=TRUE, class.source = 'fold-hide'}

ggplot(x, aes(log(sales), orders, color = category)) +
   geom_jitter(aes(shape = segment),
               alpha       = 0.7,
               width       = 0,
               height      = 0.3,
               show.legend = FALSE,
               size        = 2) +
   theme_dark_grey() +
   scale_color_manual(values = c("#fbb4ae", "#b3cde3", "#fed9a6")) +
   facet_grid(segment~category, scales = 'free_y') +
   labs(title = "Orders & Sales in log($) per Customer by Category/Segment",
        x     = 'log($)') +
   geom_curve(aes(x = 7, xend = 8.9, y = 18, yend = 16),
              data      = seth_vernon,
              curvature = -0.5,
              size      = 1,
              arrow     = arrow(length = unit(2, "mm")),
              color     = 'white',
              alpha     = 0.7) +
   geom_text(aes(x = 6.5, y = 15),
             data        = seth_vernon ,
             label       = "Seth Vernon\n15 Orders\nTotal = $8,332",
             size        = 3.5,
             show.legend = F,
             color       = 'white',
             alpha       = 0.7)

```

Their sales are spread pretty evenly between these categories.  More orders are for office supplies, but their largest sales are for technology; not surprising. They have a some loyal customers too, like Seth Vernon.  


## üßÆ Prep Data for Forecasting {#prep}

Now lets focus on sales within this time-series. If we want to forecast sales, we can't use any other variable but `order_date`. After 2018-12-30 we don't have any information about the number of orders or where those orders are coming from. We could build models using , for example, `orders` as an independent variable to predict sales, but only until 2018-12-30. After that, the models will break down since we don't have values for `orders` to predict `sales` with.

This is when we really start using `{timetk}` and `{modeltime}`. Hopefully you can appreciate how well they work alongside `{tidyverse}` and `{tidymodels}` packages. No more learning unique processes for every type of model you want to build.   
```{r, results='hide'}
sales_daily_tbl <- data_clean_tbl %>% 
                   summarise_by_time(.date_var = order_date, 
                                     .by       = 'day', 
                                     sales     = sum(sales))
```

Let's look at the daily total sales for this company over the time-series.  
```{r Daily Total Sales, fig.width = 16, fig.height = 6, class.source = 'fold-hide'}
sales_daily_tbl %>% 
   plot_time_series(.date_var     = order_date,
                    .value        = sales,
                    .interactive  = FALSE,
                    .line_color   = "#cccccc",
                    .smooth_color = "#db0000",
                    .title        = "Daily Total Sales",
                    .y_lab        = "$") +
   theme_dark_grey()
```

There are no days with 0 orders/sales. But after in our EDA we noticed there are gaps in the data. This data spans just under 4 years, but we only have 1,230 rows of daily data.  If we have a row for every day, we should have about 365 * 4 or 1,460 rows.  We need to pad these missing days so our time-series is complete. We can fill sales with 0 for these missing days.

###  Pad Missing Dates
```{r}
sales_daily_pad_tbl <- sales_daily_tbl %>% 
                       pad_by_time(order_date, 
                                   .by        = 'day', 
                                   .pad_value = 0) 

glimpse(sales_daily_pad_tbl)
```
1,458 rows, great! Now we have a full 4-year daily dataset with no missing dates. 

Our sales variable ranges from 0 to ~ 30,000 on any given day, so we'll need to transform this continuous variable. And since 0's can mess up our machine learning models, we'll use a log10 + 1 transformation, followed by standardization that centers our data around 0.  We could do this at each recipe stage of our workflows, but we can do it here once and be done with it. Lets also change sales to sales_trans.

###  Transform `sales` 
```{r}
sales_daily_pad_trans_tbl <- sales_daily_pad_tbl %>%
                             mutate(sales = log_interval_vec(sales, 
                                                             limit_lower = 0, 
                                                             offset      = 1)) %>%
                             mutate(sales = standardize_vec(sales)) %>%
                             rename(sales_trans = sales)
```
   
We need to keep track of the Standardization Parameters for when we un-transform the data at the end.  
```{r, results='hide'}
limit_lower <-  0
limit_upper <-  30918.3876
offset <-  1
std_mean_sales <- -4.61074359571939
std_sd_sales <- 2.86059320652223
```

Let's look at our new padded & transformed time-series.
```{r Daily Total Sales (Padded & Transformed), fig.width = 16, fig.height = 6, class.source = 'fold-hide'}
sales_daily_pad_trans_tbl %>% 
   plot_time_series(order_date,
                    sales_trans,
                    .interactive  = FALSE,
                    .line_color   = "#cccccc",
                    .smooth_color = "#db0000",
                    .title        = "Daily Total Sales, Padded & Transformed",
                    .y_lab        = "Sales") +
   theme_dark_grey()
```

We might want to add lags, rolling lags, and/or fourier-series features that could help our forecast. This is way to add features to regress on for the future dates we want to forecast. Currently future forecast dates have only order_date to predict with. 

Let's look at ACF/PACF plots to see if any periods look usable.        
###  ACF/PACF
```{r Lag Diagnostics, fig.width = 16, fig.height = 8, class.source = 'fold-hide'}

lag_labels <- data.frame(name  = c("ACF","ACF","ACF","ACF","ACF","PACF","PACF","PACF","PACF"),
                         label = c('7','14','21','28','357','7','14','21','28'))

sales_daily_pad_trans_tbl %>%
   plot_acf_diagnostics(.date_var = order_date, .value = sales_trans, .lags = 400,
                        .show_white_noise_bars = TRUE, .interactive = FALSE, .line_color = "#cccccc",
                        .line_size = 0.5, .white_noise_line_color = "#db0000", .point_color = "#fed9a6",
                        .point_size = 0.8) +
   theme_dark_grey() +
   geom_text(aes(label = label),
             data  = lag_labels,
             x     = c(7, 14, 21.7, 29.5, 358, 7, 15, 22, 30),
             y     = c(.53, .52, .5, .5, .38,.5, .35, .27, .25),
             size  = 4,
             color = "white",
             angle = 55)

```

Clearly there is a strong weekly correlation of sales vs sales 7d, 14d, 21d, ... later. The Partial Auto Correlation Features (PACF) de-weights the correlation values depending on the values that come before it. Our PACF shows lags 7, 14, 21, and 28 as potentially important lags.  All other lags can be considered white noise or close to white noise.  

I experimented with multiple lag and Fourier-Series combos. A lag the length of my forecast (84 days) with rolling 30d, 60d, and 90d averages helped. Additional lags and Fourier-Series did not improve model accuracy. It's possible I didn't use them correctly, or experiment enough. I'll include multiple Fourier-Series with 3 orders each to show the process.

###  Add Forecast/Lag/Rolling Averages
```{r, results='hide'}
forecast_3_month <- 84
lag_period       <- 84            # 84 smallest lag period to get our forecast of 84 into the future
rolling_periods  <- c(30, 60, 90) # 1 month, 2 month, and 3 month moving averages as features to catch the trend
```

###  Full Data
```{r}
full_data_prepared_tbl <- sales_daily_pad_trans_tbl %>% 
                          future_frame(.date_var   = order_date, 
                                       .length_out = forecast_3_month, 
                                       .bind_data  = TRUE) %>% 
                          tk_augment_fourier(.date_var = order_date,
                                             .periods  = c(7,14,21,28,357),
                                             .K        = 3) %>% 
                          tk_augment_lags(.value = sales_trans,
                                          .lags  = lag_period ) %>% 
                          tk_augment_slidify(.value   = sales_trans_lag84,
                                             .f       = ~ mean(.x, na.rm = TRUE),
                                             .period  = rolling_periods,
                                             .partial = TRUE,
                                             .align   = 'center') %>% 
                          rowid_to_column(var = 'rowid') %>% 
                          rename_with(.cols = contains('lag'), 
                                      .fn   = ~ str_c('lag_', .)) %>% 
                          rename_with(.cols = matches('(_sin)|(_cos)'), 
                                      .fn   = ~ str_c('fourier_', .))

glimpse(full_data_prepared_tbl)
```

The time-series now includes lag, rolling_lag and fourier-series data in future forecast dates. If these features help model accuracy on the training/testing sets, we can assume they'll help the models forecast future sales. They may or may not help though. Gotta experiment. Let's look at the lag and rolling lags data. 
```{r Lagged Rolling Averages, fig.width = 16, fig.height = 6, warning=FALSE, message=FALSE, class.source = 'fold-hide'}
full_data_prepared_tbl %>%
   pivot_longer(c(sales_trans,
                  lag_sales_trans_lag84,
                  lag_sales_trans_lag84_roll_30,
                  lag_sales_trans_lag84_roll_60,
                  lag_sales_trans_lag84_roll_90)) %>%
   plot_time_series(order_date, value, name, .interactive = FALSE, .smooth = FALSE, .title = "30d, 60d, 90d Rolling Lags") +
   theme_dark_grey() +
   scale_color_manual(values = c("#cccccc","#db0000", "#0099ff", "#00ff99", "#ffffcc"))
```

Notice each of the first 84 days lacks at least some of the lag/rolling lag features. We can't use these days in our testing/training splits. That's the downside. We probably could pad those dates somehow, but I won't try here. 
```{r, include=FALSE}
skimr::skim(full_data_prepared_tbl)
```

The `full_data_prepared_tbl` has 1542 rows (1458 original days + 84 future days). We need to separate rows for future days so we can split the original data into training/testing sets. Filter using days lacking `sales_trans` values.  

###  Forecast Data

We can use `skim()` from `{skimr}` to show us a summary of our data.
```{r}
forecast_tbl <- full_data_prepared_tbl %>% 
                filter(is.na(sales_trans))

skimr::skim(forecast_tbl)
```

The `forecast_tbl` has 84 rows, 37 columns for 84 future dates starting 2018-12-31 and ending 2019-03-24. Rows are only missing `sales_trans` data. Good to go.  


###  Training/Testing Data

The rest of the data we'll call the `data_prepared_tbl`. We need to drop those rows that lack any lag/rolling lag data.
```{r}
data_prepared_tbl <- full_data_prepared_tbl %>% 
                     filter(!is.na(sales_trans)) %>% 
                     drop_na()

skimr::skim(data_prepared_tbl)
```

1374 rows, 37 columns, no missing data. Dates now range from 2015-03-28 to 2018-12-30.  Good to go.  

I decided to split training/testing so we're testing on the last 84 days, the same amount of days in our `forecast_tbl`. Could have split differently. Cross-Validation for sequential models need to be ordered, so making the testing set too big limits the benefit of time-series cross-validation. That'll make sense soon.   
```{r}
splits <- data_prepared_tbl %>% 
          time_series_split(date_var   = order_date,
                            assess     = 84,
                            cumulative = TRUE)

```
```{r Initial Cross Validation Plan, fig.width = 16, fig.height = 6, message=FALSE, class.source = 'fold-hide'}
splits %>%
   tk_time_series_cv_plan() %>%
   plot_time_series_cv_plan(.date_var    = order_date,
                            .value       = sales_trans,
                            .smooth      = FALSE,
                            .interactive = FALSE,
                            .title       = "Initial Cross-Validation Plan",
                            .y_lab       = "Sales") +
   theme_dark_grey() +
   scale_color_manual(values = c("#cccccc", "#db0000"))
```

`{modeltime}` was built to complement `{tidymodels}`, so the recipes, workflows, hyperparameter tuning, etc. all work the same.

I'm going to make two recipes for modeling workflows. One recipe will include the Fourier-Series, and one will lack Fourier-Series. I could have done this differently, but it works.

The `step_timeseries_signature()` function adds many time-series features based on our date variable, `order_date`. We just have to remove features we can't use with our daily dataset, like features involving minutes, hours, etc. I also added steps to normalize numeric variables and dummy the nominal variables.
```{r, results='hide'}
recipe_spec <- recipe(sales_trans ~ ., data = training(splits)) %>% 
               step_timeseries_signature(order_date) %>% 
               update_role(rowid, new_role = 'indicator') %>% 
               step_rm(matches("(.iso)|(xts)|(hour)|(minute)|(second)|(am.pm)")) %>% 
               step_normalize(matches('(index.num)|(year)|(yday)|(qday)|(mday)|(date_day)')) %>% 
               step_dummy(all_nominal(), one_hot = TRUE) %>% 
               step_holiday(order_date, holidays = timeDate::listHolidays("US")) 
```

`recipe_spec` uses all the Fourier-Series found in our `data_prepared_tbl`.
```{r, results='hide'}
recipe_spec_no_f <- recipe(sales_trans ~ order_date 
                           + lag_sales_trans_lag84
                           + lag_sales_trans_lag84_roll_30
                           + lag_sales_trans_lag84_roll_60
                           + lag_sales_trans_lag84_roll_90, 
                           data = training(splits)) %>% 
   step_timeseries_signature(order_date) %>% 
   step_rm(matches("(.iso)|(xts)|(hour)|(minute)|(second)|(am.pm)")) %>% 
   step_normalize(matches('(index.num)|(year)|(yday)|(qday)|(mday)|(date_day)')) %>% 
   step_dummy(all_nominal(), one_hot = TRUE) %>% 
   step_holiday(order_date, holidays = timeDate::listHolidays("US")) 

```

`recipe_spec_no_f` does NOT use the Fourier-Series found in our `data_prepared_tbl`.
```{r}
recipe_spec_no_f %>% prep() %>% juice() %>% glimpse()
```

Even without Fourier-Series, look at all the features that may help us modele future sales. We started with just `order_date`.  


## üèó Build Workflows {#build}

We don't know which algorithms will work best with our data to predict sales, so lets train and test a bunch of models. Building workflows makes it so easy to move from one model to the next. Notice how many different models I tested and the code chunks are nearly identical. Experimenting becomes easy and limited only by your CPUs. 

Let's build:

* **3 sequential models:** 
   * ARIMA, ARIMA_boost, NNETAR
* **8 non-sequential models:** 
   * Prophet, XGBoost, Prophet_boost, SVM, RF, NNET, MARS, GLMNET
* **2 Recipes per model:** 
   * 1 w/ Fourier-Seires, 1 w/out Fourier-Series

Certain models use a date object like `order_date` as a predictor. Some models CANNOT and we'll predict on all variables in our recipes EXCEPT `order_date`.  

**Models that use a date object predictor:**
ARIMA, ARIMA_boost, Neral Network Autoregression (NNETAR), Prophet, and Prophet_boost

**Models that CANNOT use a date object predictor:**
XGBoost, Support-Vector Machine (SVM), Random Forest (RF), Neural Network (NNET), Multiple Adaptive Regression Splines (MARS), and Elastic-Net Regularized Generalized Linear Models (GLMNET) 

Ok, lets build!

###  ARIMA 
*Only model w/out a workflow*
```{r, results='hide', message=FALSE, cache=TRUE}
set.seed(321)
model_fit_arima <- arima_reg() %>%
   set_engine('auto_arima') %>%
   fit(sales_trans ~ order_date, data = training(splits))
```

###  ARIMA BOOST
Keep `order_date` feature
```{r, results='hide', warning=FALSE, message=FALSE, cache=TRUE}

wflw_fit_arimaboost <- workflow() %>%
   add_model(spec = arima_boost() %>% set_engine('auto_arima_xgboost')) %>%
   add_recipe(recipe_spec) %>%
   fit(training(splits))

wflw_fit_arimaboost_no_f <- workflow() %>%
   add_model(spec = arima_boost() %>% set_engine('auto_arima_xgboost')) %>%
   add_recipe(recipe_spec_no_f) %>%
   fit(training(splits))
```

###  PROPHET
Keep `order_date` feature
```{r, results='hide', warning=FALSE, message=FALSE, cache=TRUE}

wflw_fit_prophet <- workflow() %>%
   add_model(spec = prophet_reg() %>% set_engine('prophet')) %>%
   add_recipe(recipe_spec) %>%
   fit(training(splits))

wflw_fit_prophet_no_f <- workflow() %>%
   add_model(spec = prophet_reg() %>% set_engine('prophet')) %>%
   add_recipe(recipe_spec_no_f) %>%
   fit(training(splits))
```

###  XGBOOST
Set `order_date` to 'indicator'
```{r, results='hide', cache=TRUE}

wflw_fit_xgboost <- workflow() %>%
   add_model(spec = boost_tree(mode = 'regression') %>% set_engine('xgboost')) %>%
   add_recipe(recipe_spec %>% update_role(order_date, new_role = 'indicator')) %>%
   fit(training(splits))

wflw_fit_xgboost_no_f <- workflow() %>%
   add_model(spec = boost_tree(mode = 'regression') %>% set_engine('xgboost')) %>%
   add_recipe(recipe_spec_no_f %>% update_role(order_date, new_role = 'indicator')) %>%
   fit(training(splits))
```

###  PROPHET BOOST
Keep `order_date` feature
```{r, results='hide', cache=TRUE}

wflw_fit_prophet_xgboost <- workflow() %>%
   add_model(spec = prophet_boost(seasonality_daily  = FALSE,
                                  seasonality_weekly = FALSE,
                                  seasonality_yearly = FALSE) %>% set_engine('prophet_xgboost')) %>%
   add_recipe(recipe_spec) %>%
   fit(training(splits))

wflw_fit_prophet_xgboost_no_f <- workflow() %>%
   add_model(spec = prophet_boost(seasonality_daily  = FALSE,
                                  seasonality_weekly = FALSE,
                                  seasonality_yearly = FALSE) %>% set_engine('prophet_xgboost')) %>%
   add_recipe(recipe_spec_no_f) %>%
   fit(training(splits))
```
I turned Prophet seasonalities off so prophet is only used for trend. XGBoost will model seasonality with the Prophet model's residuals using the calandar features from the recipe.

###  SVM
Set `order_date` to 'indicator'
```{r, results='hide', cache=TRUE}
set.seed(321)
wflw_fit_svm <- workflow() %>%
   add_model(spec = svm_rbf(mode = 'regression') %>% set_engine('kernlab')) %>%
   add_recipe(recipe_spec %>% update_role(order_date, new_role = 'indicator')) %>%
   fit(training(splits))

wflw_fit_svm_no_f <- workflow() %>%
   add_model(spec = svm_rbf(mode = 'regression') %>% set_engine('kernlab')) %>%
   add_recipe(recipe_spec_no_f %>% update_role(order_date, new_role = 'indicator')) %>%
   fit(training(splits))
```

###  RANDOM FOREST
Set `order_date` to 'indicator'
```{r, results='hide', cache=TRUE}
set.seed(321)
wflw_fit_rf <- workflow() %>%
   add_model(spec = rand_forest(mode = 'regression') %>% set_engine('ranger')) %>%
   add_recipe(recipe_spec %>% update_role(order_date, new_role = 'indicator')) %>%
   fit(training(splits))

wflw_fit_rf_no_f <- workflow() %>%
   add_model(spec = rand_forest(mode = 'regression') %>% set_engine('ranger')) %>%
   add_recipe(recipe_spec_no_f %>% update_role(order_date, new_role = 'indicator')) %>%
   fit(training(splits))
```

###  NNET
Set `order_date` to 'indicator'
```{r, results='hide', cache=TRUE}

set.seed(321)
wflw_fit_nnet <- workflow() %>%
   add_model(spec = mlp(mode = 'regression') %>% set_engine('nnet')) %>%
   add_recipe(recipe_spec %>% update_role(order_date, new_role = 'indicator')) %>%
   fit(training(splits))

wflw_fit_nnet_no_f <- workflow() %>%
   add_model(spec = mlp(mode = 'regression') %>% set_engine('nnet')) %>%
   add_recipe(recipe_spec_no_f %>% update_role(order_date, new_role = 'indicator')) %>%
   fit(training(splits))


```

###  NNETAR
Keep `order_date` feature
```{r, results='hide', warning=FALSE, message=FALSE, cache=TRUE}
set.seed(321)
wflw_fit_nnetar <- workflow() %>%
   add_model(nnetar_reg() %>% set_engine('nnetar')) %>%
   add_recipe(recipe_spec) %>%
   fit(training(splits))

wflw_fit_nnetar_no_f <- workflow() %>%
   add_model(nnetar_reg() %>% set_engine('nnetar')) %>%
   add_recipe(recipe_spec_no_f) %>%
   fit(training(splits))
```

###  MARS
Set `order_date` to 'indicator'
```{r, results='hide', cache=TRUE}

wflw_fit_mars <- workflow() %>%
   add_model(spec = mars(mode = 'regression') %>% set_engine('earth')) %>%
   add_recipe(recipe_spec %>% update_role(order_date, new_role = 'indicator')) %>%
   fit(training(splits))

wflw_fit_mars_no_f <- workflow() %>%
   add_model(spec = mars(mode = 'regression') %>% set_engine('earth')) %>%
   add_recipe(recipe_spec_no_f %>% update_role(order_date, new_role = 'indicator')) %>%
   fit(training(splits))

modeltime_table(wflw_fit_mars, wflw_fit_mars_no_f) %>% modeltime_calibrate(testing(splits)) %>% modeltime_accuracy() %>% arrange(rmse)
```

###  GLMNET
Set `order_date` to 'indicator'
```{r, results='hide', cache=TRUE}

wflw_fit_glmnet <- workflow() %>%
   add_model(spec = linear_reg(mode = 'regression',
                               penalty = 0.1,
                               mixture = 0.5) %>% set_engine('glmnet')) %>%
   add_recipe(recipe_spec %>% update_role(order_date, new_role = 'indicator')) %>%
   fit(training(splits))

wflw_fit_glmnet_no_f <- workflow() %>%
   add_model(spec = linear_reg(mode = 'regression',
                               penalty = 0.1,
                               mixture = 0.5) %>% set_engine('glmnet')) %>%
   add_recipe(recipe_spec_no_f %>% update_role(order_date, new_role = 'indicator')) %>%
   fit(training(splits))
```

That's a lot of code, but also lots of copy/pasting followed by small changes. It's really not that bad.  Let's collect each model/workflow and measure performance on the testing data. 

**Collect and Rename**
```{r, results='hide', cache=TRUE}
submodels_tbl <- modeltime_table(model_fit_arima,
                                 wflw_fit_arimaboost,
                                 wflw_fit_arimaboost_no_f,
                                 wflw_fit_prophet,
                                 wflw_fit_prophet_no_f,
                                 wflw_fit_prophet_xgboost,
                                 wflw_fit_prophet_xgboost_no_f,
                                 wflw_fit_svm,
                                 wflw_fit_svm_no_f,
                                 wflw_fit_rf,
                                 wflw_fit_rf_no_f,
                                 wflw_fit_nnet,
                                 wflw_fit_nnet_no_f,
                                 wflw_fit_nnetar,
                                 wflw_fit_nnetar_no_f,
                                 wflw_fit_mars,
                                 wflw_fit_mars_no_f,
                                 wflw_fit_glmnet,
                                 wflw_fit_glmnet_no_f) %>% 
                 update_model_description(1, "ARIMA(0,1,1)(0,0,2)[7]") %>%
                 update_model_description(2, "ARIMA_boost") %>%
                 update_model_description(3, "ARIMA_boost-no_fourier") %>%
                 update_model_description(4, "PROPHET") %>%
                 update_model_description(5, "PROPHET-no_fourier") %>%
                 update_model_description(6, "PROPHET_boost") %>%
                 update_model_description(7, "PROPHET_boost-no_fourier") %>%
                 update_model_description(8, "SVM") %>%
                 update_model_description(9, "SVM-no_fourier") %>%
                 update_model_description(10, "RandomForest") %>%
                 update_model_description(11, "RandomForest-no_fourier") %>%
                 update_model_description(12, "NNET") %>%
                 update_model_description(13, "NNET-no_fourier") %>%
                 update_model_description(14, "NNETAR") %>%
                 update_model_description(15, "NNETAR-no_fourier") %>%
                 update_model_description(16, "MARS") %>%
                 update_model_description(17, "MARS-no_fourier") %>%
                 update_model_description(18, "GLMNET") %>%
                 update_model_description(19, "GLMNET-no_fourier")
```

**Calibrate on Testing Data**
```{r, cache=TRUE}
calibration_tbl <- submodels_tbl %>% modeltime_calibrate(testing(splits)) 
```

**Measure Accuracy**
```{r}
calibration_tbl %>% modeltime_accuracy() %>%  arrange(rmse) %>% table_modeltime_accuracy(.interactive = FALSE)
```

Looking for models with low 'rmse' and high 'rsq'.  Those perform the best fit on the testing data. I want to see how only the 6 best models perform.

**Collect 6 Best**
```{r, results='hide', cache=TRUE}
submodels_tbl_2 <- modeltime_table(wflw_fit_svm_no_f,
                                   wflw_fit_nnetar_no_f,
                                   wflw_fit_svm,
                                   wflw_fit_prophet_xgboost_no_f,
                                   wflw_fit_nnetar,
                                   wflw_fit_rf_no_f) %>% 
                   update_model_description(1, "SVM_no_f") %>% 
                   update_model_description(2, "NNETAR_no_f") %>% 
                   update_model_description(3, "SVM") %>% 
                   update_model_description(4, "PROPHET_boost_no_f") %>% 
                   update_model_description(5, "NNETAR") %>% 
                   update_model_description(6, "RandomForest_no_f") 
```

**Visualize Best 6 Models Fit on Testing Data**
```{r Models fit on testing(splits), fig.width = 16, fig.height = 6, message=FALSE}
calibration_tbl <- submodels_tbl_2 %>% modeltime_calibrate(testing(splits)) 

calibration_tbl %>%
   modeltime_forecast(new_data    = testing(splits),
                      actual_data = filter_by_time(data_prepared_tbl, .start_date = "2018-09"),
                      keep_data = TRUE) %>%
   plot_modeltime_forecast(.conf_interval_alpha = 0.02,
                           .conf_interval_fill  = 'skyblue',
                           .interactive         = FALSE,
                           .title               = "Models fit on testing(splits)",
                           .y_lab               = "sales_trans",
                           .line_size           = 1) +
   theme_dark_grey() +
   scale_color_todd_dark_bright()
```

Not bad. But we can tune these models for a better fit before forecasting.  

## üîß Hyperparameter Tuning {#tune}

There are two types of cross-validation we'll need for resampling.  K-Fold CV for models that can train on random data and Time-Series CV for models that need to train on sequential or ordered data. You'll see.  

**K-Fold CV**
```{r}
set.seed(321)
resamples_kfold <- training(splits) %>% vfold_cv(v = 6)
```
```{r K-fold CV Plan, fig.width = 16, fig.height = 7, message=FALSE, class.source = 'fold-hide'}
resamples_kfold %>%
   tk_time_series_cv_plan() %>%
   plot_time_series_cv_plan(order_date,
                            sales_trans,
                            .facet_ncol = 2,
                            .interactive = FALSE,
                            .title = "K-fold CV Plan",
                            .y_lab = "sales_trans") +
   theme_dark_grey() +
   scale_color_manual(values = c("#cccccc", "#db0000"))
```

**Time-Series CV**
```{r, message=FALSE}
set.seed(321)
resamples_tscv <- time_series_cv(data        = training(splits) %>% drop_na(),
                                 cumulative  = TRUE,
                                 assess      = "12 weeks",
                                 skip        = '6 weeks',
                                 slice_limit = 6)
```
```{r Time_Series CV Plan, fig.width = 16, fig.height = 7, message=FALSE, class.source = 'fold-hide'}
resamples_tscv %>%
   tk_time_series_cv_plan() %>%
   plot_time_series_cv_plan(order_date,
                            sales_trans,
                            .facet_ncol = 2,
                            .interactive = FALSE,
                            .title = "Time-Series CV Plan",
                            .y_lab = "sales_trans") +
   theme_dark_grey() +
   scale_color_manual(values = c("#cccccc", "#db0000"))
```

I did 2-3 rounds of tuning for all 6 models, I'll show the first and last round of tuning for just 2 of the 6 models.  We'll still compare performance of all 6 tuned models. 

Each of my tuning sets consist of:

* **Tunable Model** 
* **Workflow** 
   * specifies model and recipe
* **Parameter Grid** 
   * specifies parameters to tune and range of values to test
* **Round of Tuning**  
   * uses tunable model, workflow, parameter grid, and appropriate CV plan to train/test
* **Results Table**  
   * shows which combinations work best
* **Results Plot**  
   * chose values that associate with low rmse and high rsq

After each set I narrow the tuning grid, depending on what the plot shows me. Then I do it again until its...good...enough...? Models tuning with many tuning parameters took a 5-10 minutes to run on my 2019 16 GB, 1.6 GHz MacBook Air. I could use an upgrade. 
```{r, include=FALSE}
# PROPHET-XGBOOST TUNE
model_spec_prophet_boost_tune <- prophet_boost(changepoint_num    = tune(),
                                               changepoint_range  = 0.8,
                                               seasonality_yearly = FALSE,
                                               seasonality_weekly = FALSE,
                                               seasonality_daily  = FALSE,
                                               mtry           = 0.8,
                                               trees          = tune(),
                                               min_n          = tune(),
                                               tree_depth     = tune(),
                                               learn_rate     = tune(),
                                               loss_reduction = tune() ) %>% 
                                 set_engine('prophet_xgboost')
# Turned Prophet seasonalities off so prophet is only used for trend
# XGBoost will model seasonality with the Prophet Model's residuals using the calandar features from the recipe spec
```
```{r, include=FALSE}
wflw_spec_prophet_boost_tune <- workflow() %>% 
                                add_model(model_spec_prophet_boost_tune) %>% 
                                add_recipe(recipe_spec_no_f)
```
```{r, include=FALSE}
# Grid

set.seed(321)
grid_spec_prophet_boost_1 <- grid_latin_hypercube(parameters(model_spec_prophet_boost_tune), size = 20)
```
```{r, include=FALSE, cache=TRUE}
# Round 1
# K-fold CV + Tuning Grid 1
# Set learn_rate bc this is a costly run-time, so the more tuning parameters, the longer it takes.  

set.seed(321)
tune_results_prophet_boost_1 <- wflw_spec_prophet_boost_tune %>% 
                                tune_grid(resamples  = resamples_kfold,
                                          param_info = parameters(wflw_spec_prophet_boost_tune) %>% 
                                             update(learn_rate = learn_rate(range = c(0.001, 0.400), trans = NULL)), 
                                          grid       = grid_spec_prophet_boost_1, 
                                          control    = control_grid(verbose = TRUE))
```
```{r, include=FALSE}
# Results Table 1

tune_results_prophet_boost_1 %>% show_best('rmse', n = 10)
```
```{r, fig.width = 20, fig.height = 6, include=FALSE}
# Results Plot 1

 tune_results_prophet_boost_1 %>% 
     autoplot +
     geom_point(aes(color = '#fed9a6'), show.legend = F) +
    # geom_smooth(se = FALSE, size = 0.5) +
     theme_dark_grey() 
```
```{r, include=FALSE}
# Tuning Grid 2

set.seed(321)
grid_spec_prophet_boost_2 <- grid_latin_hypercube(changepoint_num(range = c(28,32)),
                                                  trees(range = c(1500,1525)),
                                                  min_n(range = c(13,16)),
                                                  tree_depth(range = c(4,4)),
                                                  loss_reduction(range = c(-6.5,-5.5), 
                                                                 trans = scales::log10_trans()),
                                                  learn_rate(range = c(-2.72, -2.68), 
                                                             trans = scales::log10_trans()),
                                                  size = 20)
```
```{r, include=FALSE, cache=TRUE}
# Tuning Round 2

set.seed(321)
tune_results_prophet_boost_2 <- wflw_spec_prophet_boost_tune %>% 
   tune_grid(resamples  = resamples_kfold,
             param_info = parameters(wflw_spec_prophet_boost_tune), 
             grid       = grid_spec_prophet_boost_2, 
             control    = control_grid(verbose = TRUE))
```
```{r, fig.width = 20, fig.height = 6, include=FALSE, class.source = 'fold-hide'}
tune_results_prophet_boost_2 %>% show_best('rmse', n = Inf)

 tune_results_prophet_boost_2 %>% 
     autoplot +
     geom_point(aes(color = '#fed9a6'), show.legend = F) +
     geom_smooth(se = FALSE, size = 0.5) +
     theme_dark_grey() 
# Notice the range on the Y-axis after tuning.  More tuning won't change the rmse or rsq metrics much.

```
```{r, include=FALSE, cache=TRUE}
# Fit tuned model on training data

wflw_fit_prophet_boost_tuned <- wflw_spec_prophet_boost_tune %>% 
                                finalize_workflow(select_best(tune_results_prophet_boost_2, 'rmse')) %>% 
                                fit(training(splits))
```

### SVM - no Fourier-Series

**Tunable Model**
```{r}
model_spec_svm_tune <- svm_rbf(mode      = 'regression',
                               cost      = tune(),
                               rbf_sigma = tune(),
                               margin    = 0.17 ) %>%
                       set_engine('kernlab')
```

**Workflow**

*Change `order_date` feature to 'indicator'*
```{r}
wflw_spec_svm_tune <- workflow() %>%
                      add_model(model_spec_svm_tune) %>%
                      add_recipe(recipe_spec_no_f %>% update_role(order_date,
                                                                  new_role = 'indicator'))
```

**Tuning Grid 1**
```{r}
grid_spec_svm_1 <- grid_latin_hypercube(parameters(model_spec_svm_tune),
                                                   size = 20)
```

**Tuning Round 1**
```{r, results='hide', message=FALSE, warning=FALSE, cache=TRUE}
set.seed(321)
tune_results_svm <- wflw_spec_svm_tune %>%
                    tune_grid(resamples  = resamples_kfold,
                              grid       = grid_spec_svm_1,
                              control    = control_grid(verbose = TRUE))
```

**Results Table 1**
```{r}
tune_results_svm %>% show_best('rmse', n = 10)
```

**Results Plot 1**
```{r, fig.width = 8, fig.height = 6, warning=FALSE, message=FALSE, results='hide'}
 tune_results_svm %>% 
     autoplot +
     geom_point(aes(color = '#fed9a6'), show.legend = F) +
     geom_smooth(se = FALSE, size = 0.5) +
     theme_dark_grey() 
```

Update the grid and do round 2.

**Tuning Grid 2**
```{r}
grid_spec_svm_2 <- grid_latin_hypercube(cost(range = c(3.2,3.5),
                                             trans = scales::log2_trans()),
                                        rbf_sigma(range = c(-2.45,-2.35),
                                                  trans = scales::log10_trans()),
                                        size = 20)
```

**Tuning Round 2 **
```{r, results='hide', message=FALSE, warning=FALSE, cache=TRUE}
set.seed(321)
tune_results_svm_2 <- wflw_spec_svm_tune %>%
                      tune_grid(resamples  = resamples_kfold,
                                grid       = grid_spec_svm_2,
                                control    = control_grid(verbose = TRUE))
```

**Results Table 2**
```{r}
tune_results_svm_2 %>% show_best('rmse', n = 10)
```

**Results Plot 2**
```{r, fig.width = 8, fig.height = 6, warning=FALSE, message=FALSE, results='hide'}
 tune_results_svm_2 %>% 
     autoplot +
     geom_point(aes(color = '#fed9a6'), show.legend = F) +
     geom_smooth(se = FALSE, size = 0.5) +
     theme_dark_grey() 
```

Good enough. Look at the y-axis. Another round of tuning won't make a big difference.  

**Fit Tuned Workflow on Original Traning Data**
```{r, cache=TRUE}
wflw_fit_svm_no_f_tuned <- wflw_spec_svm_tune %>%
   finalize_workflow(select_best(tune_results_svm_2, 'rmse')) %>%
   fit(training(splits))
```

```{r, include=FALSE}
## SVM w/ Fourier Series
# Tunable Model
model_spec_svm_tune_f <- svm_rbf(mode      = 'regression',
                                 cost      = tune(),
                                 rbf_sigma = tune(),
                                 margin    = 0.17) %>%
                         set_engine('kernlab')
```
```{r, include=FALSE}
## Workflow  
# Change `order_date` feature to 'indicator'

wflw_spec_svm_tune_f <- workflow() %>%
                        add_model(model_spec_svm_tune) %>%
                        add_recipe(recipe_spec %>% update_role(order_date,
                                                               new_role = 'indicator'))
```
```{r, include=FALSE}
# Tuning Grid 1

grid_spec_svm_f_1 <- grid_latin_hypercube(parameters(model_spec_svm_tune_f),
                                          size = 20)
```
```{r, results='hide', include=FALSE, cache=TRUE}
# Tuning Round 1

set.seed(321)
tune_results_svm_f <- wflw_spec_svm_tune_f %>%
                      tune_grid(resamples  = resamples_kfold,
                                grid       = grid_spec_svm_f_1,
                                control    = control_grid(verbose = TRUE))
```
```{r, include=FALSE}
# Results Table

tune_results_svm_f %>% show_best('rmse', n = 10)
```
```{r, fig.width = 8, fig.height = 6, warning=FALSE, message=FALSE, results='hide', include=FALSE}
# Results Plot

tune_results_svm_f %>% 
     autoplot +
     geom_point(aes(color = '#fed9a6'), show.legend = F) +
     geom_smooth(se = FALSE, size = 0.5) +
     theme_dark_grey() 
```
```{r, include=FALSE}
# Tuning Grid 2
grid_spec_svm_f_2 <- grid_latin_hypercube(cost(range = c(1,2),
                                               trans = scales::log2_trans()),
                                          rbf_sigma(range = c(-2.5,-2.4),
                                               trans = scales::log10_trans()),
                                          size = 10)
```
```{r, results='hide', include=FALSE, cache=TRUE}
# Tuning Round 2
set.seed(321)
tune_results_svm_f_2 <- wflw_spec_svm_tune_f %>%
                        tune_grid(resamples  = resamples_kfold,
                                  grid       = grid_spec_svm_f_2,
                                  control    = control_grid(verbose = TRUE))
```
```{r, include=FALSE}
# Results Table 2

tune_results_svm_f_2 %>% show_best('rmse', n = 10)
```
```{r, fig.width = 8, fig.height = 6, include=FALSE}
# Results Plot 2 
 tune_results_svm_f_2 %>% 
     autoplot +
     geom_point(aes(color = '#fed9a6'), show.legend = F) +
     geom_smooth(se = FALSE, size = 0.5) +
     theme_dark_grey() 
```
```{r, include=FALSE}
# Fit Tuned Workflow on Original Traning Data

wflw_fit_svm_f_tuned <- wflw_spec_svm_tune_f %>%
   finalize_workflow(select_best(tune_results_svm_f_2, 'rmse')) %>%
   fit(training(splits))
```
```{r, include=FALSE}
## RANDOM FOREST

# Tunable Model

model_spec_rf_tune <- rand_forest(mode  = 'regression',
                                  mtry  = 20,
                                  trees = tune(),
                                  min_n = tune()) %>%
                      set_engine('ranger')
```
```{r, include=FALSE}
## Workflow  
# Change `order_date` feature to 'indicator'

wflw_spec_rf_tune <- workflow() %>% 
                     add_model(model_spec_rf_tune) %>% 
                     add_recipe(recipe_spec_no_f %>% update_role(order_date, 
                                                                 new_role = 'indicator'))
```
```{r, include=FALSE}
# Tuning Grid 1

grid_spec_rf_1 <- grid_latin_hypercube(parameters(model_spec_rf_tune),
                                       size = 10)
```
```{r, include=FALSE, cache=TRUE}
# Tuning Round 1

set.seed(321)
tune_results_rf <- wflw_spec_rf_tune %>%
                   tune_grid(resamples = resamples_kfold,
                             grid      = grid_spec_rf_1,
                             control   = control_grid(verbose = TRUE))
```
```{r, include=FALSE}
# Results Table

tune_results_rf %>% show_best('rmse', n = 10)
```
```{r, fig.width = 8, fig.height = 6, warning=FALSE, message=FALSE, results='hide', include=FALSE}
# Results Plot

 tune_results_rf %>% 
     autoplot +
     geom_point(aes(color = '#fed9a6'), show.legend = F) +
     geom_smooth(se = FALSE, size = 0.5) +
     theme_dark_grey() 
```
```{r, include=FALSE}
# Tuning Grid 2 
grid_spec_rf_2 <- grid_latin_hypercube(trees(range = c(1500,1800)),
                                       min_n(range = c(35,50)),
                                       size = 10)
```
```{r, include=FALSE, cache=TRUE}
# Tuning Round 2 

set.seed(321)
tune_results_rf_2 <- wflw_spec_rf_tune %>%
                     tune_grid(resamples = resamples_kfold,
                               grid      = grid_spec_rf_2,
                               control   = control_grid(verbose = TRUE))
```
```{r, include=FALSE}
# Results Table 2 

tune_results_rf_2 %>% show_best('rmse', n = 10)

```
```{r, fig.width = 8, fig.height = 6, include=FALSE}
# Results Table 2

 tune_results_rf_2 %>% 
     autoplot +
     geom_point(aes(color = '#fed9a6'), show.legend = F) +
     geom_smooth(se = FALSE, size = 0.5) +
     theme_dark_grey() 
```
```{r, include=FALSE, cache=TRUE}
# Fit Tuned Workflow on Original Traning Data

wflw_fit_rf_tuned <- wflw_spec_rf_tune %>%
                     finalize_workflow(select_best(tune_results_rf_2, 'rmse')) %>%
                     fit(training(splits))
```

```{r, include=FALSE}
model_spec_nnetar_tune <- nnetar_reg(mode            = 'regression', 
                                     seasonal_period = 7,
                                     non_seasonal_ar = tune(),
                                     seasonal_ar     = tune(), 
                                     num_networks    = 10,
                                     hidden_units    = tune(),
                                     penalty         = tune(),                            
                                     epochs          = 50) %>% 
                          set_engine('nnetar')
```
```{r, include=FALSE}
wflw_spec_nnetar_tune <- workflow() %>% 
                         add_model(model_spec_nnetar_tune) %>% 
                         add_recipe(recipe_spec_no_f)
```
```{r, include=FALSE}
grid_spec_nnetar_1 <- grid_latin_hypercube(parameters(model_spec_nnetar_tune),
                                           size = 20)

```
```{r, message=FALSE, warning=FALSE, results='hide', cache=TRUE, include=FALSE}
set.seed(321)
tune_results_nnetar <- wflw_spec_nnetar_tune %>% 
                       tune_grid(resamples = resamples_tscv,
                                 grid      = grid_spec_nnetar_1,
                                 control   = control_grid(verbose = TRUE))
```
```{r, include=FALSE}
tune_results_nnetar %>% show_best('rmse', n = 10)
```
```{r, fig.width = 14, fig.height = 6, warning=FALSE, message=FALSE, results='hide', include=FALSE}
 tune_results_nnetar %>% 
     autoplot +
     geom_point(aes(color = '#fed9a6'), show.legend = F) +
     geom_smooth(se = FALSE, size = 0.5) +
     theme_dark_grey() 
```
```{r, cache=TRUE, include=FALSE}
set.seed(321)
grid_spec_nnetar_2 <- grid_latin_hypercube(non_seasonal_ar(range = c(1,1)),
                                           seasonal_ar(range = c(0,0)),
                                           hidden_units(range = c(4,5)),
                                           penalty(range = c(-6,-3), 
                                                   trans = scales::log10_trans()),
                                           size = 10)
```
```{r, message=FALSE, warning=FALSE, results='hide', cache=TRUE, include=FALSE}
set.seed(321)
tune_results_nnetar_2 <- wflw_spec_nnetar_tune %>% 
                         tune_grid(resamples = resamples_tscv,
                                   grid      = grid_spec_nnetar_2,
                                   control   = control_grid(verbose = TRUE))
```
```{r, include=FALSE}
tune_results_nnetar_2 %>% show_best('rmse', n = 10)
```
```{r, fig.width = 8, fig.height = 6, include=FALSE}
#Results Plot 2

tune_results_nnetar_2 %>% 
     autoplot +
     geom_point(aes(color = '#fed9a6'), show.legend = F) +
     geom_smooth(se = FALSE, size = 0.5) +
     theme_dark_grey() 
```

```{r, cache=TRUE, include=FALSE}
#Fit Tuned Workflow on Original Traning Data

wflw_fit_nnetar_no_f_tuned <- wflw_spec_nnetar_tune %>% 
                              finalize_workflow(select_best(tune_results_nnetar_2, 'rmse')) %>% 
                              fit(training(splits))
```

### NNETAR - w/ Fourier Series

**Tunable Model**
```{r}
model_spec_nnetar_tune <- nnetar_reg(mode            = 'regression', 
                                     seasonal_period = 7,
                                     non_seasonal_ar = tune(),
                                     seasonal_ar     = tune(), 
                                     num_networks    = 10,
                                     hidden_units    = tune(),
                                     penalty         = tune(),                            
                                     epochs          = 50) %>% 
                          set_engine('nnetar')
```

**Workflow **

*Keep `order_date` feature*
```{r}
wflw_spec_nnetar_tune_f <- workflow() %>% 
                           add_model(model_spec_nnetar_tune) %>% 
                           add_recipe(recipe_spec)
```

**Tuning Grid 1 **
```{r}
grid_spec_nnetar_f_1 <- grid_latin_hypercube(parameters(model_spec_nnetar_tune),
                                             size = 20)
```

**Tuning Round 1 **
```{r, message=FALSE, warning=FALSE, results='hide', cache=TRUE}
set.seed(321)
tune_results_nnetar_f <- wflw_spec_nnetar_tune_f %>% 
                         tune_grid(resamples = resamples_tscv,
                                   grid      = grid_spec_nnetar_f_1,
                                   control   = control_grid(verbose = TRUE))
```

**Results Table 1 **
```{r}
tune_results_nnetar_f %>% show_best('rmse', n = 10)
```

**Results Plot 1 **
```{r, fig.width = 14, fig.height = 6, warning=FALSE, message=FALSE, results='hide'}
 tune_results_nnetar_f %>% 
     autoplot +
     geom_point(aes(color = '#fed9a6'), show.legend = F) +
     geom_smooth(se = FALSE, size = 0.5) +
     theme_dark_grey() 
```

**Tuning Grid 2 **
```{r }
set.seed(321)
grid_spec_nnetar_f_2 <- grid_latin_hypercube(non_seasonal_ar(range = c(1,1)),
                                             seasonal_ar(range = c(0,2)),
                                             hidden_units(range = c(5,5)),
                                             penalty(range = c(-6,-3), 
                                                     trans = scales::log10_trans()),
                                             size = 10)
```

**Tuning Round 2 **
```{r, cache=TRUE}
set.seed(321)
tune_results_nnetar_f_2 <- wflw_spec_nnetar_tune_f %>% 
                           tune_grid(resamples = resamples_tscv,
                                     grid      = grid_spec_nnetar_f_2,
                                     control   = control_grid(verbose = TRUE))
```

**Results Table 2 **
```{r}
tune_results_nnetar_f_2 %>% show_best('rmse', n = 10)
```

**Results Plot 2 **
```{r, fig.width = 8, fig.height = 6}
tune_results_nnetar_f_2 %>% 
     autoplot +
     geom_point(aes(color = '#fed9a6'), show.legend = F) +
     geom_smooth(se = FALSE, size = 0.5) +
     theme_dark_grey() 
```

**Fit Tuned Workflow on Original Traning Data**
```{r, cache=TRUE}

wflw_fit_nnetar_f_tuned <- wflw_spec_nnetar_tune_f %>% 
                           finalize_workflow(select_best(tune_results_nnetar_f_2, 'rmse')) %>% 
                           fit(training(splits))
```

## üèÜ Forecast w/ Best Models {#forecast}

After lots of tuning, we have 6 models that have been tuned and fit back onto the training data. Now let's evaluate their performance on the testing data

**Collect Tuned Models**
```{r, cache=TRUE}

submodels_tuned_tbl <- modeltime_table(wflw_fit_prophet_boost_tuned,
                                       wflw_fit_svm_f_tuned,
                                       wflw_fit_svm_no_f_tuned,
                                       wflw_fit_rf_tuned,
                                       wflw_fit_nnetar_no_f_tuned,
                                       wflw_fit_nnetar_f_tuned) %>% 
                       update_model_description(1, "PROPHET_XGBOOST-tuned") %>% 
                       update_model_description(2, "SVM-fourier-tuned") %>%   
                       update_model_description(3, "SVM-no_fourier-tuned") %>% 
                       update_model_description(4, "Random Forest-tuned") %>% 
                       update_model_description(5, "NNETAR-no_fourier-tuned") %>% 
                       update_model_description(6, "NNETAR-fourier-tuned")  
```

**Calibrate on Test Data**
```{r, cache=TRUE}
calibration_tuned_tbl <- submodels_tuned_tbl %>% modeltime_calibrate(testing(splits))
```

**Measure Accuracy**
```{r, cache=TRUE}
calibration_tuned_tbl %>% modeltime_accuracy() %>% arrange(rmse) 
```

**Visualize Fit on Test Data**
```{r Tuned Models fit on testing(splits), fig.height=6, fig.width=16, message=FALSE, class.source = 'fold-hide'}
calibration_tuned_tbl %>% 
   modeltime_forecast(new_data    = testing(splits),
                      actual_data = filter_by_time(data_prepared_tbl, .start_date = "2018-09"),
                      keep_data   = TRUE) %>% 
   plot_modeltime_forecast(.conf_interval_alpha = 0.02, 
                           .conf_interval_fill  = 'skyblue', 
                           .interactive         = FALSE, 
                           .title               = "Tuned Models fit on testing(splits)",
                           .y_lab               = "sales_trans", 
                           .line_size           = 1) +
   theme_dark_grey() +
   scale_color_todd_dark_bright()
```

The hyperparameter tuning helped the performance of some models, at least regarding fit on the testing data. I could do some more work tuning, but lets move forward and forecast future sales with these tuned models. We need to refit the models on the entire dataset now, not only on training or testing data.  Once refit, we can forecast daily sales for the next 84 days. 

**Refit on All Data**
```{r, cache=TRUE}
refit_tbl <- calibration_tuned_tbl %>% 
             modeltime_refit(data = data_prepared_tbl)
```

After refitting, we can un-transform the data using those transformation parameters we saved way up top and visualize our models on a true y-axis of sales.

**Visualizing Sales Forecast with 6 Tuned-Models Zoomed In**
```{r, fig.height=8, fig.width=16, message=FALSE}
refit_tbl %>% 
   modeltime_forecast(new_data    = forecast_tbl,
                      actual_data = data_prepared_tbl,
                      keep_data   = FALSE) %>%  
   mutate(across(.value:.conf_hi, .fns = ~ standardize_inv_vec(x    = .,
                                                               mean = std_mean_sales,
                                                               sd   = std_sd_sales))) %>%
   mutate(across(.value:.conf_hi, .fns = ~ log_interval_inv_vec(x           = ., 
                                                                limit_lower = limit_lower,
                                                                limit_upper = limit_upper,
                                                                offset      = offset))) %>% 
   filter_by_time(.start_date = '2018-10' ) %>% 
   plot_modeltime_forecast(.conf_interval_alpha = 0.02, 
                           .conf_interval_fill  = 'skyblue', 
                           .interactive         = FALSE, 
                           .title               = "Tuned Models 3-Month Forecast",
                           .y_lab               = "Daily Sales ($)",
                           .line_size           = 0.8) +
   theme_dark_grey() +
   scale_color_todd_dark_bright()
```

**Visualizing Sales Forecast with 6 Tuned-Models Zoomed Out**
```{r, fig.height=8, fig.width=16, message=FALSE, class.source = 'fold-hide'}
refit_tbl %>% 
   modeltime_forecast(new_data    = forecast_tbl,
                      actual_data = data_prepared_tbl,
                      keep_data   = FALSE) %>%  
   mutate(across(.value:.conf_hi, .fns = ~ standardize_inv_vec(x    = .,
                                                               mean = std_mean_sales,
                                                               sd   = std_sd_sales))) %>%
   mutate(across(.value:.conf_hi, .fns = ~ log_interval_inv_vec(x           = ., 
                                                                limit_lower = limit_lower,
                                                                limit_upper = limit_upper,
                                                                offset      = offset))) %>% 
   #filter_by_time(.start_date = '2018-10' ) %>% 
   plot_modeltime_forecast(.conf_interval_alpha = 0.02, 
                           .conf_interval_fill  = 'skyblue', 
                           .interactive         = FALSE, 
                           .title               = "Tuned Models 3-Month Forecast",
                           .y_lab               = "Daily Sales ($)",
                           .line_size           = 0.5) +
   theme_dark_grey() +
   scale_color_todd_dark_bright()
```

## üèÅ Ensemble & Save Work {#ensemble}

There's plenty more we could do to improve these models, but the last thing I'll show here is a quick and easy weighted model. Instead of using all 6 tuned models individually, I wanted to give the best performing tuned models a scalable weight and make 1 weighted ensemble model. My best fitting model was `SVM-no_fourier-tuned`, the worst fitting was `PROPHET_XGBOOST-tuned`.  My weights ranged from 10 for the best to 5 for the worst.  

**Weighted Ensemble Model**
```{r, cache=TRUE}
calibration_tuned_tbl %>% modeltime_accuracy() %>% arrange(rmse) 
```
```{r, cache=TRUE}
weight_ensemble_tbl <- calibration_tuned_tbl %>%  
                       ensemble_weighted(loadings = c(5,7,10,6,8,9)) %>% 
                       modeltime_table()
```

**Calibrate Accuracy of Weighted Ensemble Model **
```{r, cache=TRUE}
weight_ensemble_tbl %>% modeltime_accuracy(testing(splits))
```

**Refit Ensemble on All Data**
```{r, cache=TRUE}
refit_weight_tbl <- weight_ensemble_tbl %>% 
                    modeltime_refit(data = data_prepared_tbl)
```

**Visualizing Sales Forecast with Ensemble Model**
```{r Ensemble Forecast, fig.height=8, fig.width=16, message=FALSE, class.source = 'fold-hide'}
refit_weight_tbl %>% 
   modeltime_forecast(new_data    = forecast_tbl,
                      actual_data = data_prepared_tbl)  %>%  
   mutate(across(.value, .fns = ~ standardize_inv_vec(x    = .,
                                                      mean = std_mean_sales,
                                                      sd   = std_sd_sales))) %>%
   mutate(across(.value, .fns = ~ log_interval_inv_vec(x           = ., 
                                                       limit_lower = limit_lower,
                                                       limit_upper = limit_upper,
                                                       offset      = offset))) %>% 
   #filter_by_time(.start_date = '2018') %>% 
   plot_modeltime_forecast(.interactive         = FALSE, 
                           .title               = "Weighted Ensemble Forecast", 
                           .y_lab               = "Daily Sales ($)",
                           .line_size           = 0.6) +
   theme_dark_grey() +
   scale_color_todd_dark_bright()
```

If you need the actual forecast values, be sure to un-transform and save any individual model or ensemble data. These files have a lot of information, so they can get quite large.  

**Collect Un-Transformed Forecast Data**
```{r, cache=TRUE}
forecast_tbl_final <- refit_weight_tbl %>% 
   modeltime_forecast(new_data    = forecast_tbl,
                      actual_data = data_prepared_tbl)  %>%  
   mutate(across(.value, .fns = ~ standardize_inv_vec(x    = .,
                                                      mean = std_mean_sales,
                                                      sd   = std_sd_sales))) %>%
   mutate(across(.value, .fns = ~ log_interval_inv_vec(x           = ., 
                                                       limit_lower = limit_lower,
                                                       limit_upper = limit_upper,
                                                       offset      = offset))) %>% 
  filter(.key == 'prediction')
```
```{r}
forecast_tbl_final
```

*The `.value` is our daily sales prediction for the corresponding date (`.index`).*  

In a future post I'll show how to use the `{modeltime.ensemble}` package for creating more sophisticated ensembles. Until then, I hope you've gotten a good sense for what forecasting with `{modeltime}` can do. If you already model with `{tidymodels}`, `{modeltime}` is a package worth digging into. Find the complete code @ [github.com/TWarczak](https://github.com/TWarczak/data_warz/content/post/2021-05-20-forecasting-daily-sales-w-modeltime). 

