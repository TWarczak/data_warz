---
title: "Combining AWS SageMaker + RStudio Tools; Predicting House Price using Engineered NLP Features and Multi-class XGBoost, with Beautiful Geospacial Plots and SHAP to Explain Model Behavior."
author: Todd Warczak
date: '2021-08-19'
slug: XGB-Multi-Class
categories:
  - rstats
  - AWS
  - SageMaker
  - XGBoost
  - R
  - SHAP
  - Geospacial 
tags:
  - reticulate
  - AWS
  - SageMaker
  - XGBoost
  - multiclass
  - SHAP
  - R
  - rstats
  - NLP
  - ggmaps
  - Leaflet
  - Geospacial
subtitle: 'Interpretable XGBoost with {ggmaps}, Leaflet Interactive Plots, and SHapley Additive exPlanations (SHAP)'
summary: "In this post I explore an Austin Housing dataset and predict binned housing price. EDA includes static and interactive geospacial feature maps and feature engineering using natural language processing (NLP). In my RStudio I utilize a conda environment to access SageMaker's XGBoost, S3, and EC2 for distributed computing. After model training/tuning/evaluation, I run batch inference with the best performing model to predict the price of Austin houses from holdout data. I then submit predictions to the Kaggle competition. Multiclass Log Loss was the evaluation algorithm and my submission scored 0.8876 (lower is better), which would have placed 6th in the live competition. After submission, I run SHapley Additive exPlanations (SHAP) to understand what the XGBoost model considered important features and how it clustered similar observations."
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: true
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE, 
                      echo = TRUE, dpi = 300, cache.lazy = FALSE,
                      tidy = "styler")
library(scales)
library(tidyverse)
library(tidymodels)
library(ggmap)
library(leaflet)
library(scico)
library(patchwork)
library(tidytext)
library(reticulate)    # calling the SageMaker Python SDK from R
theme_set(ggdark::dark_theme_minimal())
```

In this post I explore an Austin, TX real estate dataset and predict binned housing price. EDA includes static and interactive geospacial feature maps and feature engineering using natural language processing (NLP). In my RStudio I utilize a conda environment to access SageMaker's XGBoost, S3, and EC2 for distributed computing. After model training/tuning/evaluation I run SHapley Additive exPlanations (SHAP) to understand what the XGBoost model considered important features and how it clustered similar observations. I ran batch inference with the best performing model to predict the price of Austin houses in holdout data and then submitted predictions to the [Kaggle](https://www.kaggle.com/c/sliced-s01e11-semifinals/) competition. Multiclass Log Loss was the evaluation algorithm and my submission scored ___ (lower is better), which would have placed __ in the live competition.

## EDA

Our modeling goal is to predict  given features about the real
estate listing. This is a multiclass classification challenge, where . The main data set
provided is in a CSV file called `training.csv`.

```{r index-1}
train_raw <- read_csv("~/Documents/R/data_warz/content/post/2021-08-01-austin-r-xgb-house-price/train.csv")

holdout <- read_csv("~/Documents/R/data_warz/content/post/2021-08-01-austin-r-xgb-house-price/test.csv")
names(train_raw)
#names(holdout) # lacks priceRange
table(train_raw$priceRange)
```

Are Austin house prices clustered by neighborhoods? Do prices change near highways, airports, etc? Downtown vs. suburbs? 

We can use the `{leaflet}` package to build interactive maps overlayed with our data. Zoom to dive deeper into individual neighborhoods.

```{r index-2, fig.height=10, fig.width=10}
q <- train_raw %>%
     mutate(priceRange_bin = parse_number(priceRange))

#table(q$priceRange)
#scico::scico_palette_show(palettes = scico_palette_names())

pal <- colorNumeric(palette = scico(6, palette = 'buda'),
                    domain = q$priceRange_bin)

m <- leaflet(q) %>% 
     addTiles() %>% 
     addCircles(lng = ~longitude, lat = ~latitude, 
                color = ~pal(priceRange_bin), opacity = 0.8) %>% 
     setView(lng = -97.75, lat = 30.3, zoom = 12 ) %>% 
     addProviderTiles("CartoDB.DarkMatterNoLabels")%>%
     addProviderTiles("Stamen.TonerLines") %>%
     addProviderTiles("Stamen.TonerLabels") %>% 
     addLegend("bottomright", 
               pal       = pal, 
               values    = ~priceRange_bin,
               title     = "Price", 
               labFormat = labelFormat(prefix = "$"),
               opacity   = 1)
m
```

I want to see multiple features distributed on a similar map. Let's build hex plots for the numeric predictors and use `{patchwork}` to make one nice plot.   

```{r index-3, fig.width=10, fig.height=14}

plot_hex <- function(var, title) {
  q %>%
    ggplot(aes(longitude, latitude, z = {{var}})) +
    stat_summary_hex(bins = 40) +
    scico::scale_fill_scico(palette = 'imola') +
    labs(fill = "Avg", title = title) +
    ggdark::dark_theme_minimal()  
}

(plot_hex(priceRange_bin,"Price")      + plot_hex(avgSchoolRating, "School Rating")) /
(plot_hex(yearBuilt,"Year Built")      + plot_hex(log(lotSizeSqFt),"Lot size (log)"))/
(plot_hex(garageSpaces,"Garages")      + plot_hex(MedianStudentsPerTeacher, "Median Student/Teacher")) / 
(plot_hex(numOfBathrooms, "Bathrooms") + plot_hex(numOfBedrooms,"Bedrooms")) 
```

I'm going to drop `city` for being imballanced. I don't trust humans to tally `numOfPatioAndPorchFeatures` consistently, but we'll keep it in. Ditto for hasSpa.   

```{r index-4, fig.width=10, fig.height=8}
drop1 <- plot_hex(priceRange_bin,"Price by city") +
  facet_wrap(~city)

drop2 <- plot_hex(numOfPatioAndPorchFeatures, "# Patio Features")

drop3 <- plot_hex(priceRange_bin,"Price by hasSpa") +
  facet_wrap(~hasSpa)

(drop1 + drop2) / (drop3 + plot_spacer())
```
I always like making density plots of numeric features for classification predictions. Just another way to see if any category stands out within a feature.  

```{r index-5, fig.width=10, fig.height=10}
dense <- q %>%
         select(-c(uid, city, description, homeType, 
                   hasSpa, priceRange_bin, yearBuilt)) %>%
         pivot_longer(cols = c(garageSpaces, avgSchoolRating, MedianStudentsPerTeacher,
                               numOfBathrooms, numOfBedrooms, longitude, latitude), 
                      names_to = "feature", 
                      values_to = "value")

ggplot(dense, aes(value, fill = priceRange)) +
  geom_density(alpha = .5) +
  facet_wrap(~ feature, scales = "free") +
  labs(title = "Numeric features impacting priceRange") +
  ggdark::dark_theme_minimal() +
  theme(legend.position = c(0.45, 0.2),
        legend.background = element_rect(fill = "black", color = "white"))
```

Between the maps, hex-plots and density plots, it seems `latitude`, `longitude`, `avgSchoolRating`, and `MedianStudentsPerTeacher` will be good predictors of `priceRange`.  

## NLP - Can we find important words in `description` to engineer some features?

Let's borrow some tricks from [Julia Silge](https://juliasilge.com), the OG of text-mining in R, to identify important words and then add dummy variables for important words found in an observations `description`. 

```{r index-6}
austin_text <- 
  train_raw %>%
  mutate(priceRange = parse_number(priceRange) + 100000) %>% # make bin categories numeric so we can run glm
  unnest_tokens(word, description) %>% # Splits column into tokens, flattening table into one-token-per-row.
  anti_join(get_stopwords()) # Removes stop words

austin_text %>%
  count(word, sort = TRUE)
```

Find word frequencies per `priceRange` bin for the top 100 words.

```{r index-7}
top_words <- 
  austin_text %>%
  count(word, sort = TRUE) %>%
  filter(!word %in% as.character(1:5)) %>% # removing numbers
  slice_max(n, n = 100) %>% # 100 most frequent words
  pull(word)

word_freqs <- 
  austin_text %>%
  count(word, priceRange) %>%
  complete(word, priceRange, fill = list(n = 0)) %>%   # 0 instead of NA
  group_by(priceRange) %>%
  mutate(price_total = sum(n),
         proportion = n / price_total) %>%
  ungroup() %>%
  filter(word %in% top_words)

word_freqs
```

Build a glm that shows word frequency **increasing** across `priceRange` bins or **decreasing**
across `priceRange` bins.

```{r index-8}
word_mods <-
  word_freqs %>%
  nest(data = c(priceRange, n, price_total, proportion)) %>%
  mutate(model = map(data, ~ glm(cbind(n, price_total) ~ priceRange, ., family = "binomial")),
         model = map(model, tidy)) %>%
  unnest(model) %>%
  filter(term == "priceRange") %>% # want slope and not intercept
  mutate(adj.pvalue = p.adjust(p.value)) %>%  # adjusting p-values for imbalanced words
  arrange(-estimate)

word_mods
```

These are the words that we'd like to try to detect and use in feature engineering
for our xgboost model, rather than using all the text tokens as features
individually.

```{r index-9}
higher_words <-
  word_mods %>%
  filter(p.value < 0.05) %>%
  slice_max(estimate, n = 12) %>%
  pull(word)

lower_words <- 
  word_mods %>%
  filter(p.value < 0.05) %>%
  slice_max(-estimate, n = 12) %>%
  pull(word)
```

We can look at these changes with price directly. For example, these are the words
most associated with price decrease.

```{r index-10, fig.width=12, fig.height=14}
high <- word_freqs %>% 
  filter(word %in% higher_words) %>%
  ggplot(aes(priceRange, proportion, color = word)) +
  geom_line(size = 2.5, alpha = 0.7, show.legend = FALSE) +
  facet_wrap(vars(word), scales = "free") +
  scale_x_continuous(labels = scales::dollar) +
  scale_y_continuous(labels = scales::percent, limits = c(0, NA)) +
  labs(x = NULL, y = "% of word in home description at price",
       title = "12 Expensive Words")

low <- word_freqs %>% 
  filter(word %in% lower_words) %>%
  ggplot(aes(priceRange, proportion, color = word)) +
  geom_line(size = 2.5, alpha = 0.7, show.legend = FALSE) +
  facet_wrap(vars(word), scales = "free") +
  scale_x_continuous(labels = scales::dollar) +
  scale_y_continuous(labels = scales::percent, limits = c(0, NA)) +
  labs(x = NULL, y = "% of word in home description at price",
       title = "12 Cheap Words") 

high/low
```

Some of these words show up in a great linear trend across the `priceRange` bins. I assume XGBoost will find them useful. 

## XGBoost

Before we plug data into SageMaker's XGBoost, we need to tranform the target `priceRange` into a factor of numbers. It is NOT recommended to transform the target inside a recipe. To transform the predictors and add our engineered high/low word variables, we'll use `{tidymodels}` recipe that we can easily apply to our train/test/validation/holdout data.  

```{r index-11}
train_raw$priceRange <- factor(recode(train_raw$priceRange,
                                      "0-250000"      = "0", 
                                      "250000-350000" = "1",
                                      "350000-450000" = "2",
                                      "450000-650000" = "3", 
                                      "650000+"       = "4"), 
                               levels = c("0", "1", "2", "3", "4"))

head(train_raw$priceRange, n = 20)
```
Split data
```{r index-12}
# Make 75/12.5/12.5 Train/Validate/Test split
set.seed(333)
splits  <- initial_split(train_raw, prop = 0.75, strata = priceRange)

train <- training(splits)
other <- testing(splits)

splits2 <- initial_split(other, prop = 0.50, strata = priceRange)
validation <- training(splits2)
test <- testing(splits2)

# Before predicting holdout data, refit best models on train+validation data.
tr_val <- bind_rows(train, validation) 

print(splits)
print(splits2)
```

Let's add only the most significant high/low words. If they help the model, we could tinker with these features more.  

Drop `city` and `description`, reduce the sparse categories in `hometype` to an `other` category, then dummy both `hasSpa` and `homeType`. XGBoost only works with numeric data.

```{r index-13}
higher_words_6 <- word_mods %>%
                   filter(p.value < 0.05) %>%
                   slice_max(estimate, n = 6) %>%
                   pull(word)

lower_words_5 <- word_mods %>%
                  filter(p.value < 0.05) %>%
                  slice_max(-estimate, n = 5) %>%
                  pull(word)

higher_pat <- glue::glue_collapse(higher_words_6, sep = "|")
lower_pat <- glue::glue_collapse(lower_words_5, sep = "|")

rec_austin1 <-
  recipe(priceRange ~ ., data = train) %>%
  step_regex(description, pattern = higher_pat, result = "high_price_words") %>%
  step_regex(description, pattern = lower_pat, result = "low_price_words") %>%
  step_rm(c(uid, city, description)) %>%
  step_mutate(hasSpa = as.factor(hasSpa)) %>%
  step_novel(homeType) %>% 
  step_unknown(homeType) %>% 
  step_other(homeType, threshold = 0.02) %>%
  step_dummy(all_nominal_predictors()) %>%
  prep()
```

After running `prep()` on the training data, `bake()` applies the recipe steps to each split data. XGBoost needs the target variable `priceRange` to be the first column.  
```{r index-14}
austin_training <-  rec_austin1 %>% 
                     juice() %>% 
                     select(priceRange, everything()) # put priceRange as 1st column

austin_validation <- bake(rec_austin1, new_data = validation) %>% 
                     select(priceRange, everything())

austin_test <-       bake(rec_austin1, new_data = test) %>% 
                     select(priceRange, everything()) 

austin_holdout <-    bake(rec_austin1, new_data = holdout) 
                     # priceRange not in this data

austin_tr_val <-     bake(rec_austin1, new_data = tr_val) %>%
                     select(priceRange, everything())

id_holdout <- holdout %>% select(uid) # save for batch inference at end
```

Save all these data locally, then send them over to your S3 bucket on AWS. 
```{r index-15, eval=FALSE}
dir.create("../2021-08-01-austin-r-xgb-house-price/data") # local

write_csv(austin_training, col_names = FALSE,
          "../2021-08-01-austin-r-xgb-house-price/data/austin_training.csv")

write_csv(austin_validation, col_names = FALSE,
          "../2021-08-01-austin-r-xgb-house-price/data/austin_validation.csv")

write_csv(austin_test %>% select(-priceRange),  col_names = FALSE,
          "../2021-08-01-austin-r-xgb-house-price/data/austin_test.csv")

write_csv(austin_test, col_names = TRUE,  # Need this later to use with results
          "../2021-08-01-austin-r-xgb-house-price/data/austin_test_2.csv")
          
write_csv(austin_holdout, col_names = FALSE,
          "../2021-08-01-austin-r-xgb-house-price/data/austin_holdout.csv") 

# Need this later to use with holdout results
write_csv(id_holdout, col_names = FALSE,
          "../2021-08-01-austin-r-xgb-house-price/data/austin_holdout_id.csv") 

write_csv(austin_tr_val, col_names = FALSE,
          "../2021-08-01-austin-r-xgb-house-price/data/austin_tr_val.csv") 
```

Start up your conda environment in RStudio for running AWS SageMaker
```{r index-16, eval=FALSE}
use_condaenv("sagemaker-r", required = TRUE)
sagemaker <- import("sagemaker")
session <- sagemaker$Session()
```

Set up your S3 bucket.  
```{r index-17, eval=FALSE}
bucket <- "twarczak-sagemaker7"
project <- "austin"    # keep short. 
data_path <- paste0("s3://", bucket, "/", project, "/", "data")
models_path <- paste0("s3://", bucket, "/", project, "/", "models")
```

Send your `split` data to S3. XGBoost will find your data in the S3 bucket.  
```{r index-18, eval= FALSE}
s3_uploader <- sagemaker$s3$S3Uploader()

s3_train <- s3_uploader$upload(local_path = "../2021-08-01-austin-r-xgb-house-price/data/austin_training.csv", 
                               desired_s3_uri = data_path)

s3_validation <- s3_uploader$upload(local_path = "../2021-08-01-austin-r-xgb-house-price/data/austin_validation.csv",
                                    desired_s3_uri = data_path)

s3_test <- s3_uploader$upload(local_path = "../2021-08-01-austin-r-xgb-house-price/data/austin_test.csv", 
                              desired_s3_uri = data_path)

s3_test_full <- s3_uploader$upload(local_path = "../2021-08-01-austin-r-xgb-house-price/data/austin_test_2.csv", 
                              desired_s3_uri = data_path)

s3_holdout <- s3_uploader$upload(local_path = "../2021-08-01-austin-r-xgb-house-price/data/austin_holdout.csv", 
                                 desired_s3_uri = data_path)

s3_tr_val <- s3_uploader$upload(local_path = "../2021-08-01-austin-r-xgb-house-price/data/austin_tr_val.csv", 
                                desired_s3_uri = data_path)
```

Set up AWS credentials so RStudio can talk to AWS, grab SageMaker's built-in XGBoost, and start your EC2 instance to begin modeling.  
```{r index-19, eval=FALSE}
region <- session$boto_region_name
container <- sagemaker$image_uris$retrieve(framework = "xgboost", 
                                           region    = region, 
                                           version   = "1.3-1" )

role_arn <- Sys.getenv("SAGEMAKER_ROLE_ARN")

xgb_estimator <- sagemaker$estimator$Estimator(image_uri          = container,
                                               role               = role_arn,
                                               instance_count     = 1L,
                                               instance_type      = "ml.m4.16xlarge", 
                                               volume_size        = 50L, 
                                               output_path        = models_path,
                                               sagemaker_session  = session,
                                               use_spot_instances = TRUE,
                                               max_run            = 1800L,
                                               max_wait           = 3600L )
```

Set your first XGBoost model w/ baseline parameters. Use `softprob` multiclass because we'll need to send Kaggle a probability for each category of `priceRange`. Set `mlogloss` because that's the evaluation algorithm we'll be scored on. 
```{r chunk21, eval=FALSE}
xgb_estimator$set_hyperparameters(objective        = "multi:softprob",
                                  eval_metric      = "mlogloss",
                                  num_class        = 5L, # necessary for multiclass
                                  max_depth        = 5L, 
                                  eta              = 0.1,
                                  num_round        = 70L,
                                  colsample_bytree = 0.4,
                                  alpha            = 1L,
                                  min_child_weight = 1.1,
                                  subsample        = 0.7,
                                  gamma            = 0.01)
```

Give SageMaker models you build a unique identifier. Tell XGBoost where your data is in S3. 
```{r chunk22, eval=FALSE}
algo          <- "xgb"    # keep short
timestamp      <- format(Sys.time(), "%Y-%m-%d-%H-%M-%S") # timestamp
job_name_a     <- paste(project, algo, timestamp, sep = "-")
s3_train_input <- sagemaker$inputs$TrainingInput(s3_data = s3_train, content_type = 'csv')
s3_valid_input <- sagemaker$inputs$TrainingInput(s3_data = s3_validation, content_type = 'csv')
input_data     <- list('train'      = s3_train_input,
                       'validation' = s3_valid_input)
```

Fit first XGBoost model on training data you sent to S3 bucket. 
```{r index-20, eval=FALSE}
xgb_estimator$fit(inputs   = input_data,
                  job_name = job_name_a,
                  wait     = FALSE)
```
## Evaluate model
```{r index-21 }
training_job_stats <- session$describe_training_job(job_name = "austin-xgb-2021-08-24-19-07-45")
final_metrics_a <-  map_df(training_job_stats$FinalMetricDataList, 
                           ~tibble(metric_name = .x[["MetricName"]],
                                   value = .x[["Value"]]))
```

```{r index-22.1}
final_metrics_a
```
Evaluate model on test data
```{r index-22.2}
predictions_path_1 <- paste0(models_path, "/", "austin-xgb-2021-08-24-19-07-45", "/predictions") 

xgb_batch_predictor_1 <- xgb_estimator$transformer(instance_count     = 1L, 
                                                   instance_type      = "ml.m4.16xlarge", 
                                                   strategy           = "MultiRecord",
                                                   assemble_with      = "Line",
                                                   output_path        = predictions_path_1)
```

```{r index-23, eval=FALSE}
xgb_batch_predictor_1$transform(data         = s3_test,  # test set LACKING the TRUE `priceRange`
                                content_type = 'text/csv',
                                split_type   = "Line",
                                job_name     = "austin-xgb-2021-08-24-19-07-45",
                                wait         = FALSE)
```

Compare predictions for the test data to actual `priceRange` "truth".
Download the predicted `priceRange` for every observation.
```{r index-24}
s3_downloader <- sagemaker$s3$S3Downloader()
s3_test_predictions_path_1 <- s3_downloader$list(predictions_path_1)
 
dir.create("./predictions")
s3_downloader$download(s3_test_predictions_path_1, "./predictions")
 
test_predictions_1 <- read_csv("./predictions/austin_test.csv.out",
                                col_names = FALSE )
```

To get data in the correct format, do some wrangling. This is annoying.  
```{r index-24.2}
test_predictions_1.2 <- test_predictions_1 %>% 
                      transmute("0-250000"      = X1,
                                "250000-350000" = X2,
                                "350000-450000" = X3,
                                "450000-650000" = X4,
                                "650000+"       = X5 )

# Need to get rid of brackets
test_predictions_1.2$`0-250000` <- as.numeric(str_replace_all(test_predictions_1.2$`0-250000`,"\\[|\\]", ""))
test_predictions_1.2$`650000+` <- as.numeric(str_replace_all(test_predictions_1.2$`650000+`,"\\[|\\]", "")) 

austin_test <- read_csv("./data/austin_test_2.csv") # test set WITH the TRUE `priceRange`

test_results_1 <- austin_test %>% 
                  select(priceRange) %>% 
                  bind_cols(test_predictions_1.2) %>% 
                  rename(Truth = priceRange) %>% 
                  mutate(Truth = case_when(Truth == "0" ~ "0-250000", 
                                           Truth == "1" ~ "250000-350000",
                                           Truth == "2" ~ "350000-450000",
                                           Truth == "3" ~ "450000-650000", 
                                           Truth == "4" ~ "650000+"      ))
```

Now we can understand the results. Each row sums to 1. Each column contains the probability the observations can be classified in that `priceRange` bin.  
```{r chunk29.2}
head(test_results_1)
```

First put the highest probability `priceRange` column name (per row) into a new column called `pred_class`. That will be the `priceRange` category predictred by our model for that observation. Then set up a confusion matrix to visualize predicted `priceRange` vs. actual `priceRange`.
```{r index-25, fig.height=6, fig.width=8}
test_results_1 <- test_results_1 %>%
    mutate(pred_class = pmap_chr(across(`0-250000`:`650000+`),
                        ~ names(c(...)[which.max(c(...))])))

conf1 <- test_results_1 %>% 
  conf_mat(Truth, pred_class) %>% 
  autoplot(type = "heatmap") +
  scale_fill_scico(begin = 0.2, end = 0.7, palette = "bamako") +  
  labs(title = "Confusion Matrix - XGBoost multi-class BEFORE Tuning\n") +
  scale_x_discrete(position = "top")
conf1
```

Not a bad baseline. ROC curves aren't as useful in multi-class predictions IMO, but overall AUC is still a useful metric.  
```{r index-26, fig.width=6, fig.height=6}
auc1 <- test_results_1 %>% 
  roc_auc(as.factor(Truth), `0-250000`:`650000+`)

roc1 <- test_results_1 %>% 
  roc_curve(as.factor(Truth), `0-250000`:`650000+`) %>% 
  ggplot(aes(1-specificity, sensitivity, color = .level)) +
  geom_abline(lty = 2, color = "gray80", size = 1.5) +
  geom_path(alpha = 0.7, size = 1.1) +
  coord_equal() +
  labs(color = "priceRange",
       title = "ROC: XGBoost multi-class",
       subtitle = paste("AUC = ", auc1$.estimate)) +
  theme(legend.position = c(0.8, 0.3),
        legend.background = element_rect(fill = "black"))
roc1
```

## Hyperparameter Tuning

To build an XGBoost model better than our baseline model, we need to find a better combination of hyperparameters. First set SageMaker's estimator with the static parameters. 
```{r index-27, eval=FALSE}
xgb_estimator <- sagemaker$estimator$Estimator(image_uri          = container,
                                               role               = role_arn,
                                               instance_count     = 1L,
                                               instance_type      = "ml.m5.4xlarge", 
                                               volume_size        = 10L, 
                                               output_path        = models_path,
                                               sagemaker_session  = session,
                                               use_spot_instances = TRUE,
                                               max_run            = 1800L,
                                               max_wait           = 3600L )

xgb_estimator$set_hyperparameters(objective        = "multi:softprob",
                                  eval_metric      = "mlogloss",
                                  num_class        = 5L,
                                  alpha            = 1L ) 
```
Now set the **Tunable Parameters**.
```{r index-28, eval=FALSE}
hyperp_ranges <- list(num_round = sagemaker$tuner$IntegerParameter(500L, 800L),
                      max_depth = sagemaker$tuner$IntegerParameter(5L, 8L),
              eta              = sagemaker$tuner$ContinuousParameter(0.01,0.07),
              colsample_bytree = sagemaker$tuner$ContinuousParameter(0.5, 0.75),
              min_child_weight = sagemaker$tuner$ContinuousParameter(1.0, 7.0),
              subsample        = sagemaker$tuner$ContinuousParameter(0.5, 0.9),
              gamma            = sagemaker$tuner$ContinuousParameter(0.01, 0.4, "Logarithmic"))
```

Create `HyperparameterTuner` object. I'm running 10 models in parallel, which might be too many, but it cuts down training time. Also running a bayesian optimizer instead of hyperparameter grid. 
```{r index-29, eval=FALSE}
tuner <- sagemaker$tuner$HyperparameterTuner(estimator             = xgb_estimator,
                                             objective_metric_name = "validation:mlogloss",
                                             objective_type        = "Minimize",
                                             hyperparameter_ranges = hyperp_ranges, 
                                             strategy              = "Bayesian",
                                             max_jobs              = 250L,
                                             max_parallel_jobs     = 10L)
```

Keep everything organized. Every training/tuning/transform job is a little different. Give every job a unique identifier. You can find info about each job within SageMaker. Either in the `Training` or `Inference` tabs. 
```{r index-30, eval=FALSE}
algo <- "xgb"
timestamp <- format(Sys.time(), "%Y-%m-%d-%H-%M-%S")
job_name_b <- paste(project, algo, timestamp, sep = "-")
s3_train_input <- sagemaker$inputs$TrainingInput(s3_data = s3_train,
                                                 content_type = 'csv')
s3_valid_input <- sagemaker$inputs$TrainingInput(s3_data = s3_validation,
                                                 content_type = 'csv')
input_data <- list('train'      = s3_train_input,
                   'validation' = s3_valid_input)
```

### Start tuning job
```{r index-31, eval=FALSE}
tuner$fit(inputs   = input_data, 
          job_name = job_name_b,
          wait     = FALSE )
```

### Evaluate tuning job results
```{r index-32 }
# find the jobs names in aws services: sagemaker/training/hyperparameter tuning jobs
tuning_job_results <- sagemaker$HyperparameterTuningJobAnalytics("austin-xgb-2021-08-24-19-26-01")
tuning_results_df <- tuning_job_results$dataframe()
```

These are our best performing models.
```{r index-33 }
tuning_results_df %>% arrange(FinalObjectiveValue) %>% select(FinalObjectiveValue, everything())
```

Plot a time series chart that shows how AUC developed over 250 training jobs, tuned
by the Bayesian optimizer. Also plot the hyperparameters to see if the optimizer found the sweet spots. 
```{r index-34 }
tune_plot0 <-ggplot(tuning_results_df, aes(TrainingEndTime, FinalObjectiveValue)) +
  geom_point() +
  xlab("Time") +
  ylab(tuning_job_results$description()$TrainingJobDefinition$StaticHyperParameters$`_tuning_objective_metric`) +
  ggtitle("Hyperparameter tuning mLogloss",  
          "Progression over the period of all 250 training jobs") 

tune_plot1 <- ggplot(tuning_results_df, aes(num_round, max_depth)) +
  geom_point(aes(color = FinalObjectiveValue)) +
  scale_color_scico("mLogloss", palette = "roma", direction = 1) +
  ggtitle("Hyperparameter tuning mLogloss", 
          "Using a Bayesian strategy") 

tune_plot2 <-ggplot(tuning_results_df, aes(eta, colsample_bytree)) +
  geom_point(aes(color = FinalObjectiveValue)) +
  scale_color_scico("mLogloss", palette = "roma", direction = 1) +
  ggtitle("Hyperparameter tuning mLogloss", 
          "Using a Bayesian strategy") 

tune_plot3 <-ggplot(tuning_results_df, aes(min_child_weight, subsample)) +
  geom_point(aes(color = FinalObjectiveValue)) +
  scale_color_scico("mLogloss", palette = "roma", direction = 1) +
  ggtitle("Hyperparameter tuning mLogloss", 
          "Using a Bayesian strategy") 

tune_plot4 <-ggplot(tuning_results_df, aes(gamma, eta)) +
  geom_point(aes(color = FinalObjectiveValue)) +
  scale_color_scico("mLogloss", palette = "roma", direction = 1) +
  ggtitle("Hyperparameter tuning mLogloss", 
          "Using a Bayesian strategy") 
```

```{r index-35, fig.height=12, fig.width=12}
(tune_plot0 + tune_plot1) / (tune_plot2 + tune_plot3) / (tune_plot4 + plot_spacer())
```

Let's take the top model then evaluate on the test data. 
```{r index-36 }
best_5_tuned_models <- tuning_results_df %>%
                       arrange(FinalObjectiveValue) %>% 
                       head(n=5) 

#unnest(best_10_tuned_models) %>% select(FinalObjectiveValue, everything())

best_tuned_model <- tuning_results_df %>%
                    arrange(FinalObjectiveValue) %>% 
                    head(n=1) %>% pull(TrainingJobName)

training_job_stats <- session$describe_training_job(job_name = best_tuned_model)

final_metrics_2 <-  map_df(training_job_stats$FinalMetricDataList, 
                           ~tibble(metric_name = .x[["MetricName"]],
                                   value       = .x[["Value"]]))
```

This is how well the best model performed on the validation data. 
```{r index-37 }
final_metrics_2
```

Run the best model, `"austin-xgb-2021-08-24-19-26-01-087-8bf5c7e6"`, on the test data to predict `priceRange`. 
```{r index-38}
predictions_path_2 <- paste0(models_path, "/", "austin-xgb-2021-08-24-19-26-01-087-8bf5c7e6", "/predictions")

session$create_model_from_job("austin-xgb-2021-08-24-19-26-01-087-8bf5c7e6")

xgb_batch_predictor_2 <- sagemaker$transformer$Transformer(model_name     = "austin-xgb-2021-08-24-19-26-01-087-8bf5c7e6",
                                                           instance_count = 1L, 
                                                           instance_type  = "ml.m4.4xlarge", 
                                                           strategy       = "MultiRecord",  
                                                           assemble_with  = "Line",
                                                           output_path    = predictions_path_2)
```

Start batch prediction job. Note that I added a `-2` to the job_name below. SageMaker needs all jobs to be unique. Now this `Batch transform job` name is different than the model `Training job` name. 
```{r index-39, eval=FALSE}
xgb_batch_predictor_2$transform(data         = s3_test, # test set LACKING the TRUE `priceRange`
                                content_type = 'text/csv',
                                split_type   = "Line",
                                job_name     = "austin-xgb-2021-08-24-19-26-01-087-8bf5c7e6-2",
                                wait         = FALSE) 
```

Download the predicted `priceRange` for every observation.
```{r index-40.1}
s3_test_predictions_path_2 <- s3_downloader$list(predictions_path_2)
 
s3_downloader$download(s3_test_predictions_path_2, "./predictions")
 
test_predictions_2 <- read_csv("./predictions/austin_test.csv.out",
                               col_names = FALSE) #%>% pull(X1)
```

Transform again, annoying, but there's a payoff. 
```{r index-40.2}
test_predictions_2.1 <- test_predictions_2 %>%
                      transmute("0-250000"      = X1,
                                "250000-350000" = X2,
                                "350000-450000" = X3,
                                "450000-650000" = X4,
                                "650000+"       = X5 )
 
# Need to get rid of brackets
test_predictions_2.1$`0-250000` <- as.numeric(str_replace_all(test_predictions_2.1$`0-250000`,"\\[|\\]", ""))
test_predictions_2.1$`650000+` <- as.numeric(str_replace_all(test_predictions_2.1$`650000+`,"\\[|\\]", "")) 

austin_test <- read_csv("./data/austin_test_2.csv") # test set WITH the TRUE `priceRange`

test_results_2 <- austin_test %>%
  select(priceRange) %>%
  bind_cols(test_predictions_2.1) %>%
  rename(Truth = priceRange) %>%
  mutate(Truth = case_when(Truth == "0" ~ "0-250000",
                           Truth == "1" ~ "250000-350000",
                           Truth == "2" ~ "350000-450000",
                           Truth == "3" ~ "450000-650000",
                           Truth == "4" ~ "650000+"      ))
```

Make column for predicted `priceRange` from highest probability per row.  
```{r index-41}
test_results_2 <- test_results_2 %>%
    mutate(pred_class = pmap_chr(across(`0-250000`:`650000+`),
                        ~ names(c(...)[which.max(c(...))])))
```

Build another confusion matrix and compare it to the untuned model confusion matrix.  
```{r index-42, fig.height=6, fig.width=14}
conf2 <- test_results_2 %>% 
  conf_mat(Truth, pred_class) %>% 
  autoplot(type = "heatmap") +
  scale_fill_scico(begin = 0.2, end = 0.7, palette = "bamako") +  
  labs(title = "Confusion Matrix - XGBoost multi-class AFTER Tuning\n") +
  scale_x_discrete(position = "top")
# compare to initial model

conf1 + conf2
```

That's a big improvement. Of 1,253 observations in the `austin_test` data, the tuned model correctly classifies 782 homes, compared to 747 correct classifications by the untuned model.

What about AUC? 
```{r index-43, fig.height=6, fig.width=12}
auc2 <- test_results_2 %>% 
  roc_auc(as.factor(Truth), `0-250000`:`650000+`)

roc2 <- test_results_2 %>% 
  roc_curve(as.factor(Truth), `0-250000`:`650000+`) %>% 
  ggplot(aes(1-specificity, sensitivity, color = .level)) +
  geom_abline(lty = 2, color = "gray80", size = 1.5) +
  geom_path(alpha = 0.7, size = 1.1) +
  coord_equal() +
  labs(color = "priceRange",
       title = "ROC: XGBoost multi-class TUNED",
       subtitle = paste("AUC =", auc2$.estimate )) +
  theme(legend.position = c(0.8, 0.3),
        legend.background = element_rect(fill = "black"))

roc1 + roc2
```

The AUC impoved from 0.8737 to 0.8854.

```{r index-44, include=FALSE}
# Same process, but recipe does NOT contain high/low words from `description`. Simpler model
rec_austin3 <-
  recipe(priceRange ~ ., data = train) %>%
  step_rm(c(uid, city, description)) %>% # keep (hasSpa, numOfPatioAndPorchFeatures, homeType)
  step_mutate(hasSpa = as.factor(hasSpa)) %>%
  step_novel(homeType) %>% 
  # assign novel factor level if new data has homeType category unseen in train data
  step_unknown(homeType) %>% 
  # assign missing value to level "unknown"
  step_other(homeType, threshold = 0.02) %>%
  step_dummy(all_nominal_predictors()) %>% 
  prep()
```

```{r index-45, , include=FALSE}
austin_training3 <- rec_austin3 %>% juice() %>% select(priceRange, everything()) 
austin_validation3 <- bake(rec_austin3, new_data = validation) %>% 
                      select(priceRange, everything()) 
austin_test3 <- bake(rec_austin3, new_data = test) %>% 
                select(priceRange, everything()) 
austin_holdout3 <- bake(rec_austin3, new_data = holdout)
austin_tr_val3 <- bake(rec_austin3, new_data = tr_val) %>%
                 select(priceRange, everything())
```

```{r index-46, include=FALSE}
write_csv(austin_training3, 
          "../2021-08-01-austin-r-xgb-house-price/data/austin_training3.csv", 
          col_names = FALSE)
write_csv(austin_validation3, 
          "../2021-08-01-austin-r-xgb-house-price/data/austin_validation3.csv", 
          col_names = FALSE)
write_csv(austin_test3 %>% select(-priceRange), 
          "../2021-08-01-austin-r-xgb-house-price/data/austin_test5.csv", 
          col_names = FALSE)
write_csv(austin_test3, 
          "../2021-08-01-austin-r-xgb-house-price/data/austin_test_6.csv", 
          col_names = TRUE) # Need this later to use with results

write_csv(austin_holdout3, 
          "../2021-08-01-austin-r-xgb-house-price/data/austin_holdout3.csv", 
          col_names = FALSE) 

write_csv(austin_tr_val3, 
          "../2021-08-01-austin-r-xgb-house-price/data/austin_tr_val3.csv", 
          col_names = FALSE)
```
```{r index-47, include=FALSE}
s3_train3 <- s3_uploader$upload(local_path = "../2021-08-01-austin-r-xgb-house-price/data/austin_training3.csv", 
                               desired_s3_uri = data_path)

s3_validation3 <- s3_uploader$upload(local_path = "../2021-08-01-austin-r-xgb-house-price/data/austin_validation3.csv",
                                    desired_s3_uri = data_path)

s3_test3 <- s3_uploader$upload(local_path = "../2021-08-01-austin-r-xgb-house-price/data/austin_test5.csv", 
                              desired_s3_uri = data_path)

s3_test3_full <- s3_uploader$upload(local_path = "../2021-08-01-austin-r-xgb-house-price/data/austin_test_6.csv", 
                              desired_s3_uri = data_path)

s3_holdout3 <- s3_uploader$upload(local_path = "../2021-08-01-austin-r-xgb-house-price/data/austin_holdout3.csv", 
                              desired_s3_uri = data_path)

s3_tr_val_3 <- s3_uploader$upload(local_path = "../2021-08-01-austin-r-xgb-house-price/data/austin_tr_val3.csv", 
                              desired_s3_uri = data_path)
```
```{r index-48, include=FALSE}
xgb_estimator3 <- sagemaker$estimator$Estimator(image_uri          = container,
                                                role               = role_arn,
                                                instance_count     = 1L,
                                                instance_type      = "ml.m5.4xlarge", 
                                                volume_size        = 50L, 
                                                output_path        = models_path,
                                                sagemaker_session  = session,
                                                use_spot_instances = TRUE,
                                                max_run            = 1800L,
                                                max_wait           = 3600L)
                                                #run 10 parallel jobs of m5.4xlarge

xgb_estimator3$set_hyperparameters(objective        = "multi:softprob",
                                   eval_metric      = "mlogloss",
                                   num_class        = 5L,
                                   alpha            = 1L ) 
```
```{r index-49, include=FALSE}
hyperp_ranges2 <- list(num_round = sagemaker$tuner$IntegerParameter(500L, 800L),
                       max_depth = sagemaker$tuner$IntegerParameter(4L, 8L),
              eta              = sagemaker$tuner$ContinuousParameter(0.01,0.07),
              colsample_bytree = sagemaker$tuner$ContinuousParameter(0.5, 0.85),
              min_child_weight = sagemaker$tuner$ContinuousParameter(1.0, 7.0),
              subsample        = sagemaker$tuner$ContinuousParameter(0.5, 0.9),
              gamma            = sagemaker$tuner$ContinuousParameter(0.01, 0.5))
              #try gamma (0.01, 0.5, "Logarithmic") next time
```
```{r index-50, include=FALSE}
tuner3 <- sagemaker$tuner$HyperparameterTuner(estimator             = xgb_estimator3,
                                             objective_metric_name = "validation:mlogloss",
                                             objective_type        = "Minimize",
                                             hyperparameter_ranges = hyperp_ranges2, 
                                             strategy              = "Bayesian",
                                             max_jobs              = 200L,
                                             max_parallel_jobs     = 10L)
```
```{r index-51, include=FALSE}
algo <- "xgb"
timestamp <- format(Sys.time(), "%Y-%m-%d-%H-%M-%S")
job_name_tw1 <- paste(project, algo, timestamp, sep = "-")
s3_train_input3 <- sagemaker$inputs$TrainingInput(s3_data = s3_train3,
                                                  content_type = 'csv')
s3_valid_input3 <- sagemaker$inputs$TrainingInput(s3_data = s3_validation3,
                                                  content_type = 'csv')
input_data3 <- list('train'      = s3_train_input3,
                    'validation' = s3_valid_input3)
```
```{r index-52, include=FALSE}
tuner3$fit(inputs   = input_data3, 
           job_name = job_name_tw1,
           wait     = FALSE )
```
```{r index-53, include=FALSE}
# find the jobs names in aws services: sagemaker, training, hyperparameter tuning jobs
tuning_job_results4 <- sagemaker$HyperparameterTuningJobAnalytics("austin-xgb-2021-08-24-16-05-39")
tuning_results_df4 <- tuning_job_results4$dataframe()
```
```{r index-54, include=FALSE}
tuning_results_df4 %>% arrange(FinalObjectiveValue) %>% select(FinalObjectiveValue, everything())
```
```{r index-55, include=FALSE}
tune_plot10 <-ggplot(tuning_results_df4, aes(TrainingEndTime, FinalObjectiveValue)) +
  geom_point() +
  xlab("Time") +
  ylab(tuning_job_results4$description()$TrainingJobDefinition$StaticHyperParameters$`_tuning_objective_metric`) +
  ggtitle("Hyperparameter tuning mLogloss",  
          "Progression over the period of all 200 training jobs") 

tune_plot11 <- ggplot(tuning_results_df4, aes(num_round, max_depth)) +
  geom_point(aes(color = FinalObjectiveValue)) +
  scale_color_scico("mLogloss", palette = "roma", direction = 1) +
  ggtitle("Hyperparameter tuning mLogloss", 
          "Using a Bayesian strategy") 

tune_plot12 <-ggplot(tuning_results_df4, aes(eta, colsample_bytree)) +
  geom_point(aes(color = FinalObjectiveValue)) +
  scale_color_scico("mLogloss", palette = "roma", direction = 1) +
  ggtitle("Hyperparameter tuning mLogloss", 
          "Using a Bayesian strategy") 

tune_plot13 <-ggplot(tuning_results_df4, aes(min_child_weight, subsample)) +
  geom_point(aes(color = FinalObjectiveValue)) +
  scale_color_scico("mLogloss", palette = "roma", direction = 1) +
  ggtitle("Hyperparameter tuning mLogloss", 
          "Using a Bayesian strategy") 

tune_plot14 <-ggplot(tuning_results_df4, aes(gamma, eta)) +
  geom_point(aes(color = FinalObjectiveValue)) +
  scale_color_scico("mLogloss", palette = "roma", direction = 1) +
  ggtitle("Hyperparameter tuning mLogloss", 
          "Using a Bayesian strategy") 
```
```{r index-56, include=FALSE, fig.height=12, fig.width=12}
(tune_plot5 + tune_plot6) / (tune_plot7 + tune_plot8) / (tune_plot9 + plot_spacer())
```
```{r index-57, include=FALSE, fig.height=12, fig.width=12}
(tune_plot10 + tune_plot11) / (tune_plot12 + tune_plot13) / (tune_plot14 + plot_spacer())
```
```{r index-58, include=FALSE}
best_5_tuned_models4 <- tuning_results_df4 %>%
                    arrange(FinalObjectiveValue) %>% 
                    head(n=5) 

#unnest(best_10_tuned_models) %>% select(FinalObjectiveValue, everything())

best_tuned_model4 <- tuning_results_df4 %>%
                    arrange(FinalObjectiveValue) %>% 
                    head(n=1) %>% pull(TrainingJobName)

#best_tuned_model
training_job_stats4 <- session$describe_training_job(job_name = best_tuned_model4)

final_metrics_4 <-  map_df(training_job_stats4$FinalMetricDataList, 
                           ~tibble(metric_name = .x[["MetricName"]],
                                   value       = .x[["Value"]]))
```
```{r index-59, include=FALSE}
final_metrics_4
```
```{r index-60, include=FALSE}
predictions_path_4 <- paste0(models_path, "/", "austin-xgb-2021-08-24-16-05-39-168-ba9e4034", "/predictions")

session$create_model_from_job("austin-xgb-2021-08-24-16-05-39-168-ba9e4034")

xgb_batch_predictor_4 <- sagemaker$transformer$Transformer(model_name     = "austin-xgb-2021-08-24-16-05-39-168-ba9e4034",
                                                           instance_count = 1L, 
                                                           instance_type  = "ml.m5.4xlarge", 
                                                           strategy       = "MultiRecord",  
                                                           assemble_with  = "Line",
                                                           output_path    = predictions_path_4)
xgb_batch_predictor_4$transform(data         =  s3_test3, 
                                content_type = 'text/csv',
                                split_type   = "Line",
                                job_name     = "austin-xgb-2021-08-24-16-05-39",
                                wait         = FALSE) 
```
```{r index-61, include=FALSE}
s3_test_predictions_path_4 <- s3_downloader$list(predictions_path_4)
 
s3_downloader$download(s3_test_predictions_path_4, "./predictions")
 
test_predictions_4 <- read_csv("./predictions/austin_test5.csv.out",
                               col_names = FALSE) #%>% pull(X1)

test_predictions_5 <- test_predictions_4 %>%
                      transmute("0-250000"      = X1,
                                "250000-350000" = X2,
                                "350000-450000" = X3,
                                "450000-650000" = X4,
                                "650000+"       = X5 )
 
test_predictions_5$`0-250000` <- as.numeric(str_replace_all(test_predictions_5$`0-250000`,"\\[|\\]", ""))
test_predictions_5$`650000+` <- as.numeric(str_replace_all(test_predictions_5$`650000+`,"\\[|\\]", "")) 

austin_test3 <- read_csv("./data/austin_test_6.csv")

test_results_4 <- austin_test3 %>%
  select(priceRange) %>%
  bind_cols(test_predictions_5) %>%
  rename(Truth = priceRange) %>%
  mutate(Truth = case_when(Truth == "0" ~ "0-250000",
                           Truth == "1" ~ "250000-350000",
                           Truth == "2" ~ "350000-450000",
                           Truth == "3" ~ "450000-650000",
                           Truth == "4" ~ "650000+"      ))
```
```{r index-62, include=FALSE}
test_results_4 <- test_results_4 %>%
    mutate(pred_class = pmap_chr(across(`0-250000`:`650000+`),
                        ~ names(c(...)[which.max(c(...))])))
```
```{r index-63, include=FALSE, fig.height=12, fig.width=14}
conf4 <- test_results_4 %>% 
  conf_mat(Truth, pred_class) %>% 
  autoplot(type = "heatmap") +
  scale_fill_scico(begin = 0.2, end = 0.7, palette = "bamako") +  
  labs(title = "Confusion Matrix - XGBoost multi-class AFTER 3rd Tuning\n") +
  scale_x_discrete(position = "top")
# compare to other models

(conf1 + conf2) / (conf3 + conf4)
```
```{r index-64, include=FALSE, fig.height=10, fig.width=10}
auc4 <- test_results_4 %>% 
  roc_auc(as.factor(Truth), `0-250000`:`650000+`)

roc4 <- test_results_4 %>% 
  roc_curve(as.factor(Truth), `0-250000`:`650000+`) %>% 
  ggplot(aes(1-specificity, sensitivity, color = .level)) +
  geom_abline(lty = 2, color = "gray80", size = 1.5) +
  geom_path(alpha = 0.7, size = 1.1) +
  coord_equal() +
  labs(color = "priceRange",
       title = "ROC: XGBoost multi-class 3nd Tune",
       subtitle = paste("AUC =", auc4$.estimate )) +
  theme(legend.position = c(0.8, 0.3),
        legend.background = element_rect(fill = "black"))

(roc1 + roc2) / ( roc3 + roc4)
```

## Batch Inference on Holdout Data

First, let's refit the best tuned models on the training + validation data. 

After refitting, we need to submit probabilities of every `priceRange` bin for each observation (home). There are 5 possible price bins:
`0-250000`, `250000-350000`, `350000-450000`, `450000-650000`, `650000+`
So we'll need to submit a .cvs file containing the holdout id column + 5 price columns containing our predicted probabilities. To run inference we'll pass our "baked" holdout data into our best performing XGboost model.

```{r index-65}
best_model_spec1 <- "austin-xgb-2021-08-24-19-26-01-087-8bf5c7e6" # with high/low word features
#best_model_spec2 <- "austin-xgb-2021-08-24-16-05-39-168-ba9e4034" # without high/low words
```

Set the hyperparameters found in our best model. Search for model in SageMaker/Inference/Models and you'll find the hyperparameters within the model file.
```{r index-66}
# hyperparameters for "austin-xgb-2021-08-24-19-26-01-087-8bf5c7e6" 
xgb_estimator$set_hyperparameters(objective        = "multi:softprob",
                                  eval_metric      = "mlogloss",
                                  num_class        = 5L, # necessary for multiclass
                                  max_depth        = 6L, 
                                  eta              = 0.03520118545875153,
                                  num_round        = 547L,
                                  colsample_bytree = 0.6290542489323581,
                                  alpha            = 1L,
                                  min_child_weight = 1.1877495749360136,
                                  subsample        = 0.8303568755778328,
                                  gamma            = 0.037136763001608175)
```
Refit model on train + validation data before predicting on holdout data. 
```{r index-67, eval=FALSE}
# Create training job name based project organization principles
algo <- "xgb"    # keep short
timestamp <- format(Sys.time(), "%Y-%m-%d-%H-%M-%S") # timestamp
job_name_refit <- paste(project, algo, timestamp, sep = "-")
s3_train_input <- sagemaker$inputs$TrainingInput(s3_data = s3_tr_val,     # train + validation
                                                 content_type = 'csv')
s3_valid_input <- sagemaker$inputs$TrainingInput(s3_data = s3_test_full,  # test set w/ truth
                                                 content_type = 'csv')
input_data <- list('train'      = s3_train_input,
                   'validation' = s3_valid_input) 
```

```{r index-68 eval=FALSE}
xgb_estimator$fit(inputs   = input_data,
                  job_name = job_name_refit,
                  wait     = FALSE)
```

Our refit model name is `austin-xgb-2021-08-24-22-07-41`.
```{r index-69 }
training_job_stats_refit <- session$describe_training_job(job_name = "austin-xgb-2021-08-24-22-07-41")

final_metrics_refit <-  map_df(training_job_stats_refit$FinalMetricDataList, 
                               ~tibble(metric_name = .x[["MetricName"]],
                                       value = .x[["Value"]]))
training_job_stats_refit$TrainingJobName
```

```{r index-70}
final_metrics_refit
```

```{r index-71 }
predictions_path_h <- paste0(models_path, "/", "austin-xgb-2021-08-24-22-07-41", "/predictions") 

session$create_model_from_job("austin-xgb-2021-08-24-22-07-41")

xgb_batch_predictor_h <- sagemaker$transformer$Transformer(model_name     = "austin-xgb-2021-08-24-22-07-41",
                                                           instance_count = 1L, 
                                                           instance_type  = "ml.m5.4xlarge", 
                                                           strategy       = "MultiRecord",  
                                                           assemble_with  = "Line",
                                                           output_path    = predictions_path_h)
```

Run batch inference on holdout data. Make job_name unique with an added `-h` for holdout. 
```{r index-72 }
xgb_batch_predictor_h$transform(data         = s3_holdout, # data to predict on for Kaggle comp.
                                content_type = 'text/csv',
                                split_type   = "Line",
                                job_name     = "austin-xgb-2021-08-24-22-07-41-h",
                                wait         = FALSE)
```

Transform to format data for submission to Kaggle. 
```{r index-73 }
s3_holdout_predictions_path_h <- s3_downloader$list(predictions_path_h)

s3_downloader$download(s3_holdout_predictions_path_h, "./predictions")
 
holdout_predictions_h <- read_csv("./predictions/austin_holdout.csv.out",
                                  col_names = FALSE) #%>% pull(X1)

holdout_predictions_h.1 <- holdout_predictions_h %>%
                           transmute("0-250000"      = X1,
                                     "250000-350000" = X2,
                                     "350000-450000" = X3,
                                     "450000-650000" = X4,
                                     "650000+"       = X5 )
 
# Remove brackets
holdout_predictions_h.1$`0-250000` <- as.numeric(str_replace_all(holdout_predictions_h.1$`0-250000`,"\\[|\\]", ""))
holdout_predictions_h.1$`650000+` <- as.numeric(str_replace_all(holdout_predictions_h.1$`650000+`,"\\[|\\]", "")) 

austin_holdout_id <- read_csv("data/austin_holdout_id.csv", col_names = FALSE) %>% 
                     rename(uid = X1)
chop_submit_final <- bind_cols(austin_holdout_id, holdout_predictions_h.1)
```

```{r index-74}
write_csv(chop_submit_final, "predictions/chop_submit_final.csv")
```

This submission received a score of 0.88876 (Multiclass Log Loss), which would have placed 6th out of 90 entries in the Kaggle competition. 

To understand how this XGBoost model predicted `priceRange`, we can use SHapley Additive exPlanations (**SHAP**) to visualize feature importance for every observation, and how those features impacted the probabilities of each `priceRange` category.  SHAP is a method to interpret results from tree-based models, and can prevent your XGBoost model from falling into `Black Box` territory, where you cannot explain how the model works.

I had to adjust the `{SHAPforxgboost}` R package slightly to work for multi-class XGBoost predictions. First, rebuild the model using the `{xgboost}` R package and the model hyperparameters.

```{r index-75.1 }
library(SHAPforxgboost)
library(xgboost)

lb <- as.numeric(austin_training$priceRange) -1   # to get priceRange categories 0:4
X_train <- as.matrix(austin_training[, -1])       # remove priceRange from matrix

bst <- xgboost(data             = X_train, 
               label            = lb,
               objective        = "multi:softprob", 
               eval_metric      = "mlogloss",
               max_depth        = 6, 
               eta              = 0.03520118545875153, 
               nthread          = 2, 
               nrounds          = 547, 
               subsample        = 0.8303568755778328,
               num_class        = 5,
               gamma            = 0.037136763001608175,
               colsample_bytree = 0.6290542489323581,
               min_child_weight = 1.1877495749360136)
```
```{r index-75.2 }
# predict for softmax returns num_class probability numbers per case:
pred_labels1 <- predict(bst, as.matrix(austin_training[, -1]))
pred_labels2 <- matrix(pred_labels1, ncol=5, byrow=TRUE)
# convert the probabilities to softmax labels
pred_labels3 <- max.col(pred_labels2) - 1
#head(pred_labels2, n = 4)
#head(pred_labels3, n = 4)
```
```{r index-75.3 }
pred <- predict(bst, as.matrix(austin_training[, -1]), predcontrib = TRUE)
```

As of now `{SHAPforxgboost}` can't handle multiclass predictions. Unnesting `pred` and a small adjustment to the `shap.values()` function allows SHAP values for each category.
```{r index-75.4 }
shap_contrib_0 <- pred[[1]]
shap_contrib_1 <- pred[[2]]
shap_contrib_2 <- pred[[3]]
shap_contrib_3 <- pred[[4]]
shap_contrib_4 <- pred[[5]]
```

New `shap.values()` function.
```{r index-76}
shap.values_todd <- function(shap_contrib, X_train){

  # Add colnames if not already there (required for LightGBM)
  if (is.null(colnames(shap_contrib))) {
    colnames(shap_contrib) <- c(colnames(X_train), "BIAS")
  }

  shap_contrib <- as.data.table(shap_contrib)

  # For both XGBoost and LightGBM, the baseline value is kept in the last column
  BIAS0 <- shap_contrib[, ncol(shap_contrib), with = FALSE][1]

  # Remove baseline and ensure the shap matrix has column names
  shap_contrib[, `:=`(BIAS, NULL)]

  # Make SHAP score in decreasing order
  imp <- colMeans(abs(shap_contrib))
  mean_shap_score <- imp[order(imp, decreasing = T)]

  return(list(shap_score = shap_contrib,
              mean_shap_score = mean_shap_score,
              BIAS0 = BIAS0))
}
```
```{r index-77}
shap_values_0 <- shap.values_todd(shap_contrib = shap_contrib_0, X_train)
shap_values_1 <- shap.values_todd(shap_contrib = shap_contrib_1, X_train)
shap_values_2 <- shap.values_todd(shap_contrib = shap_contrib_2, X_train)
shap_values_3 <- shap.values_todd(shap_contrib = shap_contrib_3, X_train)
shap_values_4 <- shap.values_todd(shap_contrib = shap_contrib_4, X_train)

shap_long_0 <- shap.prep(shap_contrib = shap_values_0$shap_score, X_train)
shap_long_1 <- shap.prep(shap_contrib = shap_values_1$shap_score, X_train)
shap_long_2 <- shap.prep(shap_contrib = shap_values_2$shap_score, X_train)
shap_long_3 <- shap.prep(shap_contrib = shap_values_3$shap_score, X_train)
shap_long_4 <- shap.prep(shap_contrib = shap_values_4$shap_score, X_train)
```

Use the `shap_long` values to build SHAP Summary Plots showing global feature importance.
```{r index-78}
sh_pl_0 <-  shap.plot.summary(shap_long_0) +
            ggdark::dark_theme_minimal() +
            theme(legend.position = 'bottom') +
            labs(title = "SHAP Values for Features in XGBoost Model",
                 subtitle = "Homes Predicted to be $0 - $250,000")
sh_pl_1 <-  shap.plot.summary(shap_long_1) +
            ggdark::dark_theme_minimal() +
            theme(legend.position = 'bottom') +
            labs(title = "SHAP Values for Features in XGBoost Model",
                 subtitle = "Homes Predicted to be $250,000 - $350,000")
sh_pl_2 <-  shap.plot.summary(shap_long_2) +
            ggdark::dark_theme_minimal() +
            theme(legend.position = 'bottom') +
            labs(title = "SHAP Values for Features in XGBoost Model",
                 subtitle = "Homes Predicted to be $350,000 - $450,000")
sh_pl_3 <-  shap.plot.summary(shap_long_3) +
            ggdark::dark_theme_minimal() +
            theme(legend.position = 'bottom') +
            labs(title = "SHAP Values for Features in XGBoost Model",
                 subtitle = "Homes Predicted to be $450,000 - $650,000")
sh_pl_4 <-  shap.plot.summary(shap_long_4) +
            ggdark::dark_theme_minimal() +
            theme(legend.position = 'bottom') +
            labs(title = "SHAP Values for Features in XGBoost Model",
                 subtitle = "Homes Predicted to be $650,000+")
```

```{r index-79, fig.height=25, fig.width=10}
#library(patchwork)
sh_pl_0 / sh_pl_1 / sh_pl_2 / sh_pl_3 / sh_pl_4
```

The SHAP summary plots above combine feature importance with feature effects. Each point on the summary plot is a Shapley value for an individual observation's feature. The position on the y-axis is determined by the feature importance within that `priceRange`. 

Notice feature importance values in the `$650,000` plot (5th plot) are highest among the `priceRange` categories and the `$350,000 - $450,000` plot (3rd plot) are lowest. That means our XGBoost model is better at predicting the most expensive homes and relatively poor at predicting middle priced homes. The model even predicts `$0-$250,000` homes well considering there are fewest observations of that `priceRange`

```{r index-79.2}
# Count of homes per priceRange category.  0 = $0-$250,000    4 = $650,000
table(austin_training$priceRange)
```

The color represents the relative value of the feature to the observation. 

Deep purple points with POSITIVE SHAP Values (right of 0) are feature values that strongly influence XGBoost to POSITIVELY classify the observation into THAT `priceRange` bin. 

Deep purple points with NEGATIVE SHAP Values (left of 0) are feature values that strongly influence XGBoost to NOT POSITIVELY classify the observation into THAT `priceRange` bin.   

Each feature gets it's own yellow-purple scale (can't compare purple points of different features). 

Different observations with the same feature value can have very different SHAPley values. For example, we might see a dark purple point and a bright yellow point in `numofBedrooms` feature for two homes that each have `numofBedrooms` = 3. Imagine a suburban home with 3 bedrooms being priced slightly more than a suburban 2 bedroom home. A downtown apartment, however, with 3 bedrooms might be 2X the price of a 2 bedroom downtown apartment. Therefore, the feature value (color) for `numofBedrooms` will be  higher for the apartment observation. 

Look at `high_price_words` and `low_price_words` in the `$0-$250,000` SHAP summary plot. Observations with `low_price_words` in the `$0-$250,000` plot are always purple and positive, influencing XGBoost to say YES, classify me as `$0-$250,000`. Observations with `high_price_words` are purple and negative, saying NO, don't classify me as `$0-$250,000`. The opposite pattern is seen with high/low words in the `$640,000` plot.  

The color scale for `lotSizeSqft` is messed up due to outliers.
```{r index-79.3}
austin_training %>% arrange(desc(lotSizeSqFt)) %>% head(n=50) %>% select(priceRange, lotSizeSqFt)
```
`lotSizeSqFt` range is fairly linear until the largest 9 observations. Because these 9 observations do not associate with `priceRange`, we might consider a rule removing any observation above 400,000 SqFt. These might be typos or a large empty stretch of private land. Must dig deeper into these 9 observations to adjust our model recipe.    

The last plots are SHAP Force Plots which clusters (ward.D2) similar observations and stacks SHAP values for each observation. These show how the final output was obtained as a sum of each predictors attributions. 

Think of these plots as the algorithm summing up an observations feature values to determine whether to classify into this `priceRange` bin. I'll show code for just one of the plots. 
```{r index-80, fig.height=6, fig.width=10}
shap_force_plot4 <- shap.prep.stack.data(shap_contrib = shap_values_4$shap_score, 
                                        top_n = 4, n_groups = 5)
# you may choose to zoom in at a location, and set y-axis limit using `y_parent_limit`  
shap.plot.force_plot(shap_force_plot4, zoom_in = FALSE) +
  ggdark::dark_theme_bw() +
  labs(title = "SHAP Force Plot;  $650,000+",
       subtitle = "Stacked SHAP values for each observation contribute to XGBoost assigning probability of this priceRange.")
```

```{r index-81, echo=FALSE, fig.height=6, fig.width=10}
shap_force_plot3 <- shap.prep.stack.data(shap_contrib = shap_values_3$shap_score, 
                                        top_n = 4, n_groups = 5)
# you may choose to zoom in at a location, and set y-axis limit using `y_parent_limit`  
shap.plot.force_plot(shap_force_plot3, zoom_in = FALSE) +
  ggdark::dark_theme_bw() +
  labs(title = "SHAP Force Plot; $450,000 - $650,000",
       subtitle = "Stacked SHAP values for each observation contribute to XGBoost assigning probability of this priceRange.")
```

```{r index-82, echo=FALSE, fig.height=6, fig.width=10}
shap_force_plot2 <- shap.prep.stack.data(shap_contrib = shap_values_2$shap_score, 
                                        top_n = 4, n_groups = 5)
# you may choose to zoom in at a location, and set y-axis limit using `y_parent_limit`  
shap.plot.force_plot(shap_force_plot2, zoom_in = FALSE) +
  ggdark::dark_theme_bw() +
  labs(title = "SHAP Force Plot; $350,000 - $450,000",
       subtitle = "Stacked SHAP values for each observation contribute to XGBoost assigning probability of this priceRange.")
```

```{r index-83, echo=FALSE, fig.height=6, fig.width=10}
shap_force_plot1 <- shap.prep.stack.data(shap_contrib = shap_values_1$shap_score, 
                                        top_n = 4, n_groups = 5)
# you may choose to zoom in at a location, and set y-axis limit using `y_parent_limit`  
shap.plot.force_plot(shap_force_plot1, zoom_in = FALSE) +
  ggdark::dark_theme_bw() +
  labs(title = "SHAP Force Plot; $250,000 - $350,000",
       subtitle = "Stacked SHAP values for each observation contribute to XGBoost assigning probability of this priceRange.")
```

```{r index-84, echo=FALSE, fig.height=6, fig.width=10}
shap_force_plot0 <- shap.prep.stack.data(shap_contrib = shap_values_0$shap_score, 
                                        top_n = 4, n_groups = 5)
# you may choose to zoom in at a location, and set y-axis limit using `y_parent_limit`  
shap.plot.force_plot(shap_force_plot0, zoom_in = FALSE) +
  ggdark::dark_theme_bw() +
  labs(title = "SHAP Force Plot; $0 - $250,000",
       subtitle = "Stacked SHAP values for each observation contribute to XGBoost assigning probability of this priceRange.")
```

```{r index-85, include=FALSE fig.height=6, fig.width=10}
shap_force_plot4 <- shap.prep.stack.data(shap_contrib = shap_values_4$shap_score, 
                                        top_n = 4, n_groups = 5)
# you may choose to zoom in at a location, and set y-axis limit using `y_parent_limit`  
shap.plot.force_plot(shap_force_plot4, zoom_in_group = 1, y_parent_limit = c(-5.2,5.0)) +
  ggdark::dark_theme_bw() +
  labs(title = "SHAP Force Plot;  $650,000+",
       subtitle = "Stacked SHAP values for each observation contribute to XGBoost assigning probability of this priceRange.")
```

```{r index-86, include=FALSE, fig.height=10, fig.width=10}
shap.plot.force_plot_bygroup(shap_force_plot) +
  ggdark::dark_theme_bw() +
  labs(title = "SHAP Force Plot Clusters - Austin Neighborhoods?")
```

## Summary

In this post I used a Kaggle dataset for Austin, TX real estate and predict homes into one of 5 `priceRange` bins. I performed exploratory data analysis in R, leaning heavily on geospacial tools like `{ggmaps}`, `{leaflet}` for interactive plotting, and `{ggplot2}` for distribution hex plots to visualize features across Austin. I performed natural language processing (NLP) on the `description` feature to produce high and low values words that associate with home price. I then connected RStudio to AWS SageMaker to gain access to it's built-in XGBoost, S3, and EC2 instances for modeling. After training and tuning XGBoost to find the best multi-class model, I ran batch inference on Kaggle holdout data and predicted `priceRange` probabilites for 4,961 Austin homes. I submitted those predictions to the [SLICED](https://www.kaggle.com/c/sliced-s01e11-semifinals) Kaggle Competition which scored 0.88876 (Multiclass Log Loss) which would have been good enough for 6th place (out of 90 entries). Finally, I replicate that XGBoost model in R with `{xgboost}` and build a set of multi-class XGBoost SHAP plots to better understand how the XGBoost model classified observations, based on feature importance, relative feature impact, and clustering of observations by SHAPley values. 
